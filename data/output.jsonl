{"title":"Is there a debugging guide / TRACE / detailed debugging one can get?","descriptionHtml":"<p>I'm trying to figure out why my exec jobs are hanging on Azure. My setup/run works fine on GCP, but I need more detailed logs to debug the issue.</p>","answerHtml":"<p>For user job logs, you can check <code>sky logs &#x3C;cluster-name> &#x3C;job-id></code>. Additionally, you can run <code>ray job list</code> on the VM to help debug the issue. It seems that the python version on your Azure cluster is Python 2.7.18, which might be causing the problem. You can try upgrading to the latest version of SkyPilot to see if that solves the issue.</p>"}
{"title":"How to manage skypilot SSH keys and cloud credentials for CI/Github actions?","descriptionHtml":null,"answerHtml":"<p>To manage skypilot SSH keys and cloud credentials for CI/Github actions, you can follow these steps:</p>\n<ol>\n<li>For cloud credentials, determine which cloud provider you are using (e.g., GCP, AWS, Azure).</li>\n<li>Set up the necessary environment variables or configuration files for the cloud provider in your CI/Github actions workflow.</li>\n<li>For skypilot SSH keys, you can create and store them securely in your CI/Github actions secrets or encrypted files.</li>\n<li>In your deployment workflow, use the appropriate commands or tools to configure the SSH keys and cloud credentials for skypilot.</li>\n</ol>\n<p>Here's an example for GCP:</p>\n<pre><code class=\"hljs language--v\">-v \"$HOME/.ssh/sky-key.pub:/root/.ssh/sky-key.pub\" \\\n-v \"$HOME/.ssh/sky-key:/root/.ssh/sky-key\" \\\n-v \"$HOME/.sky:/root/.sky\" ```</code></pre>"}
{"title":"Encountering cluster provisioning issue with Skypilot and Kubernetes","descriptionHtml":null,"answerHtml":"<p>It seems that the issue is related to the kubelet not running or being unhealthy. The error message suggests checking the kubelet status using the <code>systemctl status kubelet</code> command and inspecting its logs using <code>journalctl -xeu kubelet</code>. Additionally, it's recommended to check for any crashed or exited control plane components and list all running Kubernetes containers using the <code>crictl</code> command. The user who reported the issue mentioned using AWS EC2 instance with AWS Linux. Another user tried to replicate the issue but was unable to do so on a similar environment. They shared their command logs and suggested running <code>export SKYPILOT_DEBUG=1</code> before running any <code>sky</code> commands for more detailed logs. They also mentioned installing SkyPilot, docker, kubectl, and kind on the machine. It would be helpful for the user to share their command line logs after spinning up a VM to identify the specific command that fails.</p>"}
{"title":"Does the YAML allow for registering multiple models?","descriptionHtml":"<p>Is this the idea behind the multiple nodes approach?</p>","answerHtml":"<p>Yes, you can register multiple models in YAML. The multiple nodes approach allows you to run multiple models on a single VM. You can either use multiple YAML files, one for each model, or use a single YAML file and use <code>&#x26;</code> to put each model worker to background in the <code>run</code> commands.</p>"}
{"title":"What is the issue with running a task using SkyPilot on a local cluster and how to solve it?","descriptionHtml":null,"answerHtml":"<p>The issue is that the 'cloud', 'region', and 'instance_type' parameters must be set by the optimizer. To solve this, you can follow the recommended way of utilizing on-prem resources by installing Kubernetes on your on-prem cluster and using SkyPilot's Kubernetes support to submit tasks. If you're on GCP, you can refer to the provided guide to set up a Kubernetes cluster on GCP VMs and submit tasks to it using SkyPilot. Note that GPU support is currently in-progress and only single-node CPU jobs are supported in the master branch.</p>"}
{"title":"Can I use skypilot On-Prem to work with a VM acquired from a cloud not supported by skypilot?","descriptionHtml":null,"answerHtml":"<p>Yes, you can use skypilot On-Prem to work with a VM acquired from a cloud not supported by skypilot. However, the current On-Prem support has been deprecated and is no longer supported. Instead, it is recommended to use the new and experimental Kubernetes support. This will allow you to run SkyPilot tasks on a Kubernetes cluster. To try it out, you will need to deploy a Kubernetes cluster on your VM and follow the provided steps. Note that this feature is under active development and some functionalities may not be supported yet.</p>"}
{"title":"What could be causing the performance issues when launching with Skypilot?","descriptionHtml":"<p>The user is experiencing performance issues when launching with Skypilot and wants to know the possible causes.</p>","answerHtml":"<p>The performance issues could be related to I/O issues on the user's side, specifically with their SSD storage. The user noticed that their SSD storage was running low and after freeing up space, the performance improved.</p>"}
{"title":"Do we need to have sudo rights in the provided nodes in order for skypilot to work?","descriptionHtml":null,"answerHtml":"<p>Yes, sudo rights are required in certain places for SkyPilot to work. Most places where sudo is used are in cluster setup commands and handling file mounts/storage mounting. If you are using a custom image where sudo access is not given, you can try leaving out sudo in the setup commands and see if they can be executed in the image. However, it is recommended to have sudo rights for proper functionality.</p>"}
{"title":"How can a data scientist connect to the Ray head node directly from their training script without logging into the EC2 instance?","descriptionHtml":null,"answerHtml":"<p>Yes, it is possible for a data scientist to connect to the Ray head node directly from their training script without logging into the EC2 instance. They can use the <code>sky exec</code> command followed by the cluster name and the desired command, or they can use the <code>ssh</code> command followed by the cluster name. For example, <code>sky exec myclus -- echo hi</code> or <code>ssh myclus</code>.</p>"}
{"title":"How to add another kernel to Jupyter running in Skypilot?","descriptionHtml":null,"answerHtml":"<p>To add another kernel to Jupyter running in Skypilot, you can follow these steps:</p>\n<ol>\n<li>Install the <code>ipykernel</code> package in your conda environment using <code>conda install ipykernel</code>.</li>\n<li>Run the command <code>python -m ipykernel install --user --name=myenv</code> to install the kernel.</li>\n</ol>\n<p>Please note that you may need to restart Jupyter after installing the kernel. If you are using the cloud provider's deep learning image, it is recommended to start your own Jupyter notebook to have more control over the environment.</p>"}
{"title":"How can I obtain a list of all the servers running across different clouds using SkyPilot?","descriptionHtml":"<p>Is there a way to do this? I'm interested not only in the servers that are registered in the SkyPilot database and have a cluster_name, but in all servers associated with my credentials.</p>","answerHtml":"<p>Obtaining a list of all servers running across different clouds using SkyPilot is currently out of the project's scope. However, you can explore other tools like cloud_enum (https://github.com/initstring/cloud_enum) which might help you achieve this. It's important to consider privacy concerns as many organizations have non-Sky and Sky resources living in the same account.</p>"}
{"title":"","descriptionHtml":null,"answerHtml":"<p>Great job on the project! Looking forward to reading your blog post.</p>"}
{"title":"ValueError: Cloud 'Kubernetes' is not a valid cloud among ['aws', 'azure', 'gcp', 'lambda', 'local', 'ibm', 'scp', 'oci']","descriptionHtml":null,"answerHtml":"<p>This error is likely caused by branch switching. Running <code>sky check</code> again should clear it up.</p>"}
{"title":"Why is the 'ports' field unsupported in my YAML file when creating a cluster using SkyPilot?","descriptionHtml":null,"answerHtml":"<p>The 'ports' field is currently unsupported in the version you are using. To get this feature, you can install SkyPilot from source or use the nightly build. After launching the cluster on AWS, the specified ports will be accessible outside the public cloud, and inbound rules will be automatically added to the security group used by the instance.</p>"}
{"title":"Why is `sky launch` failing to connect to the launched AWS EC2 instance?","descriptionHtml":"<p>Any idea why all of a sudden <code>sky launch</code> fails to connect to the launched AWS EC2 instance?</p>","answerHtml":"<p>There could be multiple reasons why <code>sky launch</code> is failing to connect to the launched AWS EC2 instance. One possible reason is that there is an issue with the SSH connection. Another reason could be that there is a problem with the security group configuration. It is also possible that there is an issue with the Skypilot version being used. Upgrading to the latest version may resolve the issue. Additionally, there could be a problem with the AWS account permissions. Checking and ensuring that all necessary permissions are set up for the AWS account is recommended. Finally, it is worth noting that cleaning up any dangling resources and reattempting the launch may help resolve the issue.</p>"}
{"title":"Error when trying to deploy a cluster with SkyPilot","descriptionHtml":"<p>I am getting a <code>RuntimeError: Fail to build pip wheel for SkyPilot</code> error when trying to deploy a cluster with SkyPilot. Has anyone encountered this issue before?</p>","answerHtml":"<p>The issue was caused by a bug in the nightly build of SkyPilot. The bug has been fixed and you can try installing the latest version using <code>pip install -U \"skypilot-nightly>=1.0.0.dev20230901\"</code>. The bug was related to the naming of the wheel file. You can find more details about the bug and the fix in this <a href=\"https://github.com/skypilot-org/skypilot/pull/2502\" rel=\"nofollow\" target=\"_blank\">PR</a>.</p>"}
{"title":"Error running spot jobs with Skypilot","descriptionHtml":null,"answerHtml":"<p>The error you encountered is a <code>TypeError</code> that occurs when a bytes-like object is required, but a string is provided. This issue has already been addressed and fixed in the Skypilot repository. To resolve this, you can update your Skypilot version to the latest nightly build, which includes the fix. Alternatively, you can wait for the next official release, as the fix will be included in it. Upgrading to the nightly build or the next release should resolve the issue and prevent the timeout error you experienced.</p>"}
{"title":"How can I specify the Ray/Python version that SkyPilot installs?","descriptionHtml":"<p>I'm launching my training jobs from a docker container using Skypilot, but it seems that the Ray cluster needs to be running the exact same Python/Ray versions. How can I specify the versions that Skypilot installs?</p>","answerHtml":"<p>You can create a new conda environment and start the Ray cluster head using the new conda environment. Then, all the workers can connect to the cluster using the new environment as well. Here's an example snippet:</p>\n<pre><code class=\"hljs language-yaml\"><span class=\"hljs-attr\">setup:</span> <span class=\"hljs-string\">|\n  conda activate myray\n  if [ $? != \"0\" ]; then\n    conda create -n myray -y python=${PYTHON_VERSION}\n    conda activate myray\n  fi\n  pip install &#x3C;https://s3-us-west-2.amazonaws.com/ray-wheels/master/${RAY_COMMIT}/ray-3.0.0.dev0-cp38-cp38-manylinux2014_x86_64.whl>\n</span>\n  <span class=\"hljs-comment\"># works for single node (if you need to do multiple nodes, we can talk more about it)</span>\n  <span class=\"hljs-string\">ray</span> <span class=\"hljs-string\">start</span> <span class=\"hljs-string\">--head</span>\n\n<span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n  # you should be able to connect to your own ray cluster in your program\n  conda activate myray\n  python myprogram.py\n</span></code></pre>"}
{"title":"","descriptionHtml":null,"answerHtml":"<p>The team spent a lot of time ensuring that the VM and job state transitions are handled correctly and guarded against preemptions. As a result, the Managed Spot Job should be reasonably hardened now.</p>"}
{"title":"Does Skypilot currently only support the following regions? Can Skypilot support more regions?","descriptionHtml":null,"answerHtml":"<p>Yes, Skypilot currently supports the following regions: 'centralus, eastus, eastus2, northcentralus, southcentralus, westcentralus, westus, westus2, westus3'. However, it can support more regions by adding them to the list of supported regions in the code.</p>"}
{"title":"How can I quickly get the price of all VM instances across all clouds and regions?","descriptionHtml":null,"answerHtml":"<p>You can use the <code>sky show-gpus -a</code> command to get the pricing of all instances across all clouds. If you're looking for a specific GPU type, you can run <code>sky show-gpus &#x3C;gpu-type></code>. The command will display the pricing information for each instance, including the cloud, instance type, device memory, vCPUs, host memory, hourly price, hourly spot price, and region.</p>"}
{"title":"How can I query a SkyPilot node's IP address using the Python SDK?","descriptionHtml":null,"answerHtml":"<p>You can use the following command to query a SkyPilot node's IP address using the Python SDK:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> sky\nip_address = sky.status(<span class=\"hljs-string\">'vllm-llama2'</span>)[<span class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'handle'</span>].head_ip</code></pre>"}
{"title":"How can I solve the 'wait_ready timeout exceeded' error when running sky spot launch?","descriptionHtml":null,"answerHtml":"<p>This error usually occurs due to a networking issue. Here are a few steps you can take to troubleshoot:</p>\n<ol>\n<li>Check network connectivity to any AWS IP and ensure that VPN is not required.</li>\n<li>Verify if any networking or VPC settings are set in ~/.sky/config.yaml.</li>\n<li>Try running 'sky launch' instead of 'sky spot launch' to see if it works.</li>\n</ol>\n<p>If the issue persists, you can try the following:</p>\n<ol>\n<li>Check if you can SSH into the IP directly using the command 'ssh ubuntu@&#x3C;IP_ADDR> -i ~/.ssh/sky-key'.</li>\n<li>Update SkyPilot to the latest version (0.4 or skypilot-nightly) and try launching the cluster again.</li>\n</ol>\n<p>If none of these steps resolve the issue, you can try launching an AWS instance manually from the console and see if you can SSH into it. If that works, it could indicate a firewall issue on your local machine.</p>\n<p>Please note that the key should be added directly to the authorized keys in the VM by SkyPilot, rather than using a key pair.</p>\n<p>If you are still facing issues, it is recommended to reach out to the SkyPilot support team for further assistance.</p>"}
{"title":"Why is the binding key not showing up in the console when creating an instance?","descriptionHtml":null,"answerHtml":"<p>The binding key will not be shown in the console for security reasons. Instead, it is directly added to the <code>~/.ssh/authorized_keys</code> file on the VM. You can verify if the key is correctly set on the VM by SSHing into the VM through the AWS console and checking the <code>~/.ssh/authorized_keys</code> file.</p>"}
{"title":"Is there a development branch for doing multinode batch jobs on k8s?","descriptionHtml":null,"answerHtml":"<p>Yes, there is a development branch for doing multinode batch jobs on k8s. Some changes that need to be made include creating a headless service for the head node pod, using the service IP on the workers to connect to the head node pod, and potentially modifying the scheduling code to use service IPs instead of pod IPs.</p>"}
{"title":"What is the idle policy for the cluster in Skypilot?","descriptionHtml":null,"answerHtml":"<p>The idle policy for the cluster in Skypilot is based on the completion of the last job in the job queue. Once the last job is completed, the node is considered idle.</p>"}
{"title":"Does skypilot support specifying the VNet when spawning an instance on Azure?","descriptionHtml":null,"answerHtml":"<p>Currently, skypilot does not support specifying the VNet when spawning an instance on Azure. However, the necessary plumbing to make it work involves adding a vpc/vnet field in the node provider configuration and passing it to the underlying networking/instance launch call to Azure.</p>"}
{"title":"How do I let skypilot create random bucket names instead of specifying them myself?","descriptionHtml":"<p>When a spot experiment finished, it deletes the bucket, so if another spot experiment is currently running and needs to access said bucket it will fail instead.</p>","answerHtml":"<p>If the bucket is for data/code from the local disk, you can specify them without explicitly giving the bucket name, and SkyPilot will create buckets with random name and delete them after spot job finishes. For the mount mode files, you can set the bucket name to <code>$MY_BUCKET_NAME</code> and pass a random bucket name using <code>sky spot launch --env MY_BUCKET_NAME=my-bucket-$(date +%s)</code>.</p>"}
{"title":"Is there a plan for supporting Databricks?","descriptionHtml":null,"answerHtml":"<p>Yes, there is potential for supporting Databricks. Databricks is good at running data-intensive jobs, but there is a need for running compute-intensive tasks on small/medium-sized datasets. Integrating SkyPilot with Databricks can provide the solution for this. It is already possible to run Ray on Databricks, which opens up the possibility for further integration.</p>"}
{"title":"Which cloud provider is the easiest to get A100s in your experience?","descriptionHtml":null,"answerHtml":"<p>In my experience, GCP (Google Cloud Platform) is the easiest cloud provider to get A100 GPUs. They can be found in regions like us-east4/5. AWS training instances offered through SageMaker are also a good option for getting A100 GPUs on demand.</p>"}
{"title":"What is the Skypilot orchestrator in ZenML?","descriptionHtml":null,"answerHtml":"<p>The Skypilot orchestrator is a new addition to the ZenML framework. It allows users to run ZenML pipelines backed by Skypilot, providing more flexibility and access to resources on any cloud. It is expected to become popular in the ML community.</p>"}
{"title":"Recipe or wrapper around SkyPilot for serving LLM inferences at scale?","descriptionHtml":null,"answerHtml":"<p>Yes, there is a serving layer on SkyPilot that is currently in production use for Chatbot Arena with approximately 500k monthly users. You can find more technical details and information about becoming a pilot user in this link: <a href=\"https://skypilot-org.slack.com/archives/C03HUKKBQ7R/p1692917046577609\" rel=\"nofollow\" target=\"_blank\">Skypilot Slack Discussion</a></p>"}
{"title":"Any plans to support runpod through sky?","descriptionHtml":null,"answerHtml":"<p>Yes, RunPod is currently contributing support through a pull request on GitHub. You can find more information and contribute by commenting and +1ing on the pull request: <a href=\"https://github.com/skypilot-org/skypilot/pull/2360\" rel=\"nofollow\" target=\"_blank\">link</a></p>"}
{"title":"How do SSH keys work in skypilot?","descriptionHtml":"<p>Why is there an SSH key name?</p>","answerHtml":"<p>When instances are created in skypilot, the skypilot public key <code>~/.ssh/sky-key.pub</code> on your local machine is added to <code>~/.ssh/authorized_keys</code> on the remote machine. The SSH key name is used to identify the key file on the local machine and is specified in the <code>~/.ssh/config</code> file. This allows you to easily SSH onto the cluster head using just the cluster head name.</p>"}
{"title":"Who would be the right people to chat with regarding potential partnership opportunities to run SkyPilot with CoreWeave's GPU labelled on-demand nodes?","descriptionHtml":null,"answerHtml":"<p>You can reach out to Rahul Talari at CoreWeave to discuss potential partnership opportunities for running SkyPilot with their GPU labelled on-demand nodes.</p>"}
{"title":"What is the VLLM discord?","descriptionHtml":null,"answerHtml":"<p>The VLLM discord is a platform for communication and collaboration among VLLM community members. It is used to discuss VLLM-related topics, share resources, and seek help from other members. It is likely that the weird error you are experiencing can be resolved by reaching out to @Woosuk Kwon on the VLLM discord.</p>"}
{"title":"Is EFS supported for mounts?","descriptionHtml":null,"answerHtml":"<p>EFS is not natively supported for mounts, but it can be achieved using a few commands. Here is a <a href=\"https://gist.github.com/concretevitamin/eec319cea8b2bac3ac03f23f111c0561\" rel=\"nofollow\" target=\"_blank\">gist</a> that provides the steps to do it.</p>"}
{"title":"What tools can integrate well with Skypilot to achieve ML infrastructure features?","descriptionHtml":null,"answerHtml":"<p>Skypilot can integrate well with ClearML, WandB, or other ML experiments tracking tools. It also supports full reproducibility with tools like DVC. Checkpointing is supported using private object storage buckets. Skypilot enables distributed training to train models in a shorter time. It also allows users to save costs by using spot instances. For re-triggering old experiments, users can easily rerun a job using the YAML file. Skypilot currently does not have a GUI.</p>"}
{"title":"Issue launching V100 on AWS with Deep Learning AMI GPU PyTorch 2.0.1","descriptionHtml":"<p>I'm encountering an issue when trying to launch a V100 on AWS with the 'Deep Learning AMI GPU PyTorch 2.0.1'. The instance starts momentarily and then is immediately stopped. How can I resolve this?</p>","answerHtml":"<p>The issue is caused by SkyPilot hard-coding the SSH user as 'ubuntu', while the image expects logins from 'ec2-user'. To resolve this, you can temporarily hardcode the 'ec2-user' as the SSH user.</p>"}
{"title":"How can I deploy an open-source LLM for my team?","descriptionHtml":null,"answerHtml":"<p>To deploy an open-source LLM for your team, you can follow the guide provided by Tyler Dunn in the first version of the deployment guide. You can find the guide at this link: <a href=\"https://github.com/continuedev/deploy-os-code-llm/\" rel=\"nofollow\" target=\"_blank\">https://github.com/continuedev/deploy-os-code-llm/</a>. If you have any feedback or need assistance, you can reach out to Tyler Dunn or Zhanghao Wu (skypilot).</p>"}
{"title":"Parsing output of 'sky show-gpus --all >>out.txt' to a dataframe","descriptionHtml":null,"answerHtml":"<p>Yes, you can parse the output of 'sky show-gpus --all >>out.txt' to a dataframe. One way to do this is by directly dealing with the CSV files in the <code>~/.sky/catalogs</code> directory. Alternatively, you can use the <code>service catalog</code> python API provided by Skypilot to query the CSV files. Here's an example:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> sky\n\nsky.clouds.service_catalog.get_accelerator_hourly_cost(acc_name=<span class=\"hljs-string\">'A100'</span>, acc_count=<span class=\"hljs-number\">4</span>, use_spot=<span class=\"hljs-literal\">False</span>, clouds=<span class=\"hljs-string\">'gcp'</span>)</code></pre>"}
{"title":"How can I reuse an existing EC2 instance for the head instance in managed spot runs?","descriptionHtml":"<p>Is there a way to reuse one of the EC2 instances already running or specify resources for the head instance?</p>","answerHtml":"<p>For managed spot runs, you can specify the resources requirement of the controller in <code>~/.sky/config.yaml</code> file. You can set the number of CPUs required for the controller by adding the following configuration:</p>\n<pre><code class=\"hljs language-spot:\">  controller:\n    resources:\n      cpus: 4```\n\nTo make the configuration take effect, you need to stop the current controller using `sky down sky-spot-controller-&#x3C;hash>` and then launch it again using `sky spot launch` command. The controller name can be found in the output of `sky status` command.\n\nAdditionally, for multiple managed spot jobs, they will reuse the same controller node, so the cost for it is amortized.</code></pre>"}
{"title":"AWS deployment timeout issue","descriptionHtml":null,"answerHtml":"<p>The AWS deployment timeout issue could be caused by an incorrect SSH key configuration. You can try the SSH command mentioned in the provision.log to see if it hangs. If it does, you may need to check the steps suggested in this thread: <a href=\"https://skypilot-org.slack.com/archives/C03J2KQQZSS/p1695345105283099?thread_ts=1695276478.957079&#x26;cid=C03J2KQQZSS\" rel=\"nofollow\" target=\"_blank\">AWS Deployment SSH Key Configuration</a></p>"}
{"title":"Does Skypilot support non US zones on Azure?","descriptionHtml":null,"answerHtml":"<p>Yes, Skypilot supports non US zones on Azure. You can refer to the SkyPilot documentation for more information on how to configure and use Skypilot with global regions.</p>"}
{"title":"Is there a Sky Server example for Mistral 7b?","descriptionHtml":null,"answerHtml":"<p>Yes, you can find a YAML for deploying Mistral with Sky Serve in this gist: <a href=\"https://gist.github.com/infwinston/f1b797b8fc200c0c165d2e9e3bdf057d\" rel=\"nofollow\" target=\"_blank\">link</a>. Additionally, you can refer to the guide <a href=\"https://docs.google.com/document/d/1vVmzLF-EkG3Moj-q47DQBGvFipK4PNfkz0V6LyaPstE/edit\" rel=\"nofollow\" target=\"_blank\">here</a> for more information. The skyserve branch is going to be merged soon to master.</p>"}
{"title":"Recovering spot instance job taking too much time","descriptionHtml":"<p>Am I doing anything wrong?</p>","answerHtml":"<p>It would be helpful to check the detailed reasons for provisioning failures in the controller logs. This can be done using the <code>sky spot logs --controller</code> command. It is also recommended to check the optimizer table and failover sequence in the logs to identify any potential issues.</p>"}
{"title":"Is there a way to set custom SKYPILOT_TASK_ID?","descriptionHtml":null,"answerHtml":"<p>We currently don’t provide such a way, but you might be able to parse the TASK_ID in your bash script or program.</p>"}
{"title":"What's the best way to store AWS ECR credentials?","descriptionHtml":null,"answerHtml":"<p>One way to store AWS ECR credentials is by specifying an executable command to the environment variable, so that the password will be queried for each recovery. This can be done by setting the <code>SKYPILOT_DOCKER_PASSWORD</code> environment variable to <code>$(aws ecr get-login-password --region us-west-2)</code>. Another option is to pass the AWS secret key using the <code>--env</code> flag, but it may not work when using a Docker image in the <code>resources</code> section.</p>"}
{"title":"What could be causing the SSH error and termination of the VM in skypilot integration with zenml?","descriptionHtml":null,"answerHtml":"<p>The SSH error could be the cause of the termination of the VM. In some experimental settings, a similar error was observed. It is recommended to check the SSH keys, manually run variants of the SSH command, and verify the instance's SSH key in the AWS console.</p>"}
{"title":"Can I see my team's clusters and perform actions on them using SkyPilot?","descriptionHtml":"<p>I want to know if I can view and manage my team's clusters on the same cloud subscription using SkyPilot.</p>","answerHtml":"<p>Yes, you can see your team's clusters and perform actions on them using SkyPilot. In a team setting, full visibility can be obtained through the cloud console. An admin or team lead user can log into the cloud provider (e.g., Azure) and see everything in the subscription. You can also use the cloud console to debug issues by SSHing or rsyncing files from your machine to someone on your team's cluster. If a team member goes on vacation, you can stop their cluster using the recommended Autostop feature. SkyPilot supports tagging instances, so you can easily filter and find clusters based on tags. Additionally, you can distribute common YAML files using GitHub or other version control systems alongside the project code. This allows team members to share and use the same YAML files. If you have any specific requirements or need further assistance, feel free to discuss them.</p>"}
{"title":"Is there a debugging guide / TRACE / detailed debugging one can get?","descriptionHtml":"<p>I'm trying to figure out why my exec jobs are hanging on Azure. My setup/run works fine on GCP, but I need more detailed logs to debug the issue.</p>","answerHtml":"<p>For user job logs, you can check <code>sky logs &#x3C;cluster-name> &#x3C;job-id></code>. Additionally, you can run <code>ray job list</code> on the VM to help debug the issue. It seems that the python version on your Azure cluster is Python 2.7.18, which might be causing the problem. You can try upgrading to the latest version of SkyPilot to see if that solves the issue.</p>"}
{"title":"What is the VLLM discord?","descriptionHtml":null,"answerHtml":"<p>The VLLM discord is a platform for communication and collaboration among VLLM community members. It is used to discuss VLLM-related topics, share resources, and seek help from other members. It is likely that the weird error you are experiencing can be resolved by reaching out to @Woosuk Kwon on the VLLM discord.</p>"}
{"title":"Controller error after ~21 hours: 'Controller's latest status is INIT; jobs will not be shown until it becomes UP'. What could be the issue?","descriptionHtml":"<p>I'm running into this error after having the controller be up for ~21 hours. I can ssh into the machine and I also see all sorts of errors in the logs from gcp, though nothing obvious. Does anyone know what this could be? Also, is there any way to restart the controller when it's in a bad state?</p>","answerHtml":"<p>To get the spot controller out of abnormal state, you can try <code>sky start -f sky-spot-controller-&#x3C;hash></code>.</p>"}
{"title":"AWS deployment timeout issue","descriptionHtml":null,"answerHtml":"<p>The AWS deployment timeout issue could be caused by an incorrect SSH key configuration. You can try the SSH command mentioned in the provision.log to see if it hangs. If it does, you may need to check the steps suggested in this thread: <a href=\"https://skypilot-org.slack.com/archives/C03J2KQQZSS/p1695345105283099?thread_ts=1695276478.957079&#x26;cid=C03J2KQQZSS\" rel=\"nofollow\" target=\"_blank\">AWS Deployment SSH Key Configuration</a></p>"}
{"title":"What's the best way to store AWS ECR credentials?","descriptionHtml":null,"answerHtml":"<p>One way to store AWS ECR credentials is by specifying an executable command to the environment variable, so that the password will be queried for each recovery. This can be done by setting the <code>SKYPILOT_DOCKER_PASSWORD</code> environment variable to <code>$(aws ecr get-login-password --region us-west-2)</code>. Another option is to pass the AWS secret key using the <code>--env</code> flag, but it may not work when using a Docker image in the <code>resources</code> section.</p>"}
{"title":"Is there a Sky Server example for Mistral 7b?","descriptionHtml":null,"answerHtml":"<p>Yes, you can find a YAML for deploying Mistral with Sky Serve in this gist: <a href=\"https://gist.github.com/infwinston/f1b797b8fc200c0c165d2e9e3bdf057d\" rel=\"nofollow\" target=\"_blank\">link</a>. Additionally, you can refer to the guide <a href=\"https://docs.google.com/document/d/1vVmzLF-EkG3Moj-q47DQBGvFipK4PNfkz0V6LyaPstE/edit\" rel=\"nofollow\" target=\"_blank\">here</a> for more information. The skyserve branch is going to be merged soon to master.</p>"}
{"title":"When is it safe to close my computer after launching regular or managed spot jobs?","descriptionHtml":"<p>Does it matter if I am launching on an existing cluster or a new one? Do I need to wait to see logs from my setup command, or is there an earlier point in time where a cloud instance has my yaml and I can log off?</p>","answerHtml":"<p>Assuming no <code>--detach-run</code> or <code>--detach-setup</code> is used, it is safe to ctrl-c after the job’s submitted. One indicator of this is <code>INFO: Tip: use Ctrl-C to exit log streaming (task will not be killed).</code> If <code>--detach-setup</code> or <code>--detach-run</code> flags are used, you can safely ctrl-c to detach from logging without interrupting the setup process. To see the logs again after detaching, use <code>sky logs</code>. To cancel setup, cancel the job via <code>sky cancel</code>. Useful for long-running setup commands.</p>"}
{"title":"Running multiple containers using skypilot with docker-compose on a single GPU node","descriptionHtml":null,"answerHtml":"<p>Yes, you can run multiple containers using skypilot with docker-compose on a single GPU node. In the <code>run</code> section of your skypilot YAML file, you can have multiple <code>docker run</code> commands or use <code>docker compose up</code> if you have a docker-compose file. Here's an example:</p>\n<pre><code class=\"hljs language-run:\">  docker run ... &#x26;amp; \n  docker run ...```\n\nAlternatively, you can directly use `docker compose up` in the `run` section. See [example here](https://github.com/skypilot-org/skypilot/pull/2745/files).</code></pre>"}
{"title":"Skypilot stuck in infinite loop when reusing cluster on AWS","descriptionHtml":null,"answerHtml":"<p>The issue you're experiencing with Skypilot getting stuck in an infinite loop when reusing a cluster on AWS is likely due to a lack of permission for the <code>ec2:DescribeInstances</code> action. This error occurs when the ray cluster on the remote VM encounters an unauthorized operation. To fix this, you need to ensure that your AWS account has the necessary permissions. You can check your IAM user's permissions and make sure it has the <code>ec2:DescribeInstances</code> permission. Additionally, you can try using the latest <code>skypilot-nightly</code> version, which includes a new provisioner that may resolve the issue. If you're using a different IAM user or profile on your client, you'll need to ensure that the VM has access to the correct AWS credentials. You can upload your local static AWS credentials to the VM and let the service on the VM use those credentials. If you're using an AWS profile, you may need to set the <code>AWS_PROFILE</code> environment variable in the VM through Skypilot configuration. Please refer to the Skypilot documentation for more details.</p>"}
{"title":"Using Docker support with Google's Cloud Repository","descriptionHtml":null,"answerHtml":"<p>The Docker support with Google's Cloud Repository is not officially supported yet. However, you can use a workaround by using <code>docker pull</code> and <code>docker run</code> in the <code>setup</code> and <code>run</code> sections of your task YAML. Here are the steps:</p>\n<ol>\n<li>Run <code>gcloud auth configure-docker &#x3C;your-location>.pkg.dev</code> locally.</li>\n<li>Add the following section in your task YAML:</li>\n</ol>\n<pre><code class=\"hljs language-file_mounts:\">  ~/.docker/config.json: ~/.docker/config.json```\n\n3. Launch your task using `sky launch`.</code></pre>"}
{"title":"Availability of (spot) GPU quota across different clouds","descriptionHtml":"<p>Does anyone have experience they could share about the availability of (spot) GPU quota across different clouds?</p>","answerHtml":"<p>We did observe that the availability for spot on GCP is much better than on-demand ones, especially for high-end GPUs, e.g., A100, A100-80GB. For on-demand GPUs, it seems the availability is quite volatile, and it seems GCP has the best availability among the big clouds, while some other clouds, such as Oracle or IBM cloud can also have some GPUs available. We are also adding support for coreweave, fluidstack and runpod, so more GPUs to come.</p>"}
{"title":"What are the possible options or optimizations for reducing IO cost when using Trainings with pytorch and skypilot?","descriptionHtml":null,"answerHtml":"<p>There are several options and optimizations you can try to reduce IO cost when using Trainings with pytorch and skypilot:</p>\n<ol>\n<li>Consider using S3 for storage, which is generally cheaper than EFS. The exact cost depends on your data size and workload's access patterns.</li>\n<li>Use <code>COPY</code> mode in Sky Storage when using S3 with skypilot. This will maximize performance and reduce cost by pre-fetching the files to the VM's local disk.</li>\n<li>To avoid egress costs when moving data across regions, place your data in the region where you typically launch your jobs. Alternatively, you can use Cloudflare R2, which does not charge any egress fee and is cheaper than S3.</li>\n<li>EBS can also be considered as an option, especially if you are sticking to one region and zone.</li>\n<li>Unfortunately, I'm not familiar with Ray Dataset, so I cannot comment on its cost.</li>\n</ol>\n<p>Overall, S3 is the most well-tested and recommended option for reducing IO cost.</p>"}
{"title":"What could be causing the SSH error and termination of the VM in skypilot integration with zenml?","descriptionHtml":null,"answerHtml":"<p>The SSH error could be the cause of the termination of the VM. In some experimental settings, a similar error was observed. It is recommended to check the SSH keys, manually run variants of the SSH command, and verify the instance's SSH key in the AWS console.</p>"}
{"title":"How can I reuse an existing EC2 instance for the head instance in managed spot runs?","descriptionHtml":"<p>Is there a way to reuse one of the EC2 instances already running or specify resources for the head instance?</p>","answerHtml":"<p>For managed spot runs, you can specify the resources requirement of the controller in <code>~/.sky/config.yaml</code> file. You can set the number of CPUs required for the controller by adding the following configuration:</p>\n<pre><code class=\"hljs language-spot:\">  controller:\n    resources:\n      cpus: 4```\n\nTo make the configuration take effect, you need to stop the current controller using `sky down sky-spot-controller-&#x3C;hash>` and then launch it again using `sky spot launch` command. The controller name can be found in the output of `sky status` command.\n\nAdditionally, for multiple managed spot jobs, they will reuse the same controller node, so the cost for it is amortized.</code></pre>"}
{"title":"Is there a plan for supporting Databricks?","descriptionHtml":null,"answerHtml":"<p>Yes, there is potential for supporting Databricks. Databricks is good at running data-intensive jobs, but there is a need for running compute-intensive tasks on small/medium-sized datasets. Integrating SkyPilot with Databricks can provide the solution for this. It is already possible to run Ray on Databricks, which opens up the possibility for further integration.</p>"}
{"title":"Does Skypilot currently only support the following regions? Can Skypilot support more regions?","descriptionHtml":null,"answerHtml":"<p>Yes, Skypilot currently supports the following regions: 'centralus, eastus, eastus2, northcentralus, southcentralus, westcentralus, westus, westus2, westus3'. However, it can support more regions by adding them to the list of supported regions in the code.</p>"}
{"title":"How can I specify the Ray/Python version that SkyPilot installs?","descriptionHtml":"<p>I'm launching my training jobs from a docker container using Skypilot, but it seems that the Ray cluster needs to be running the exact same Python/Ray versions. How can I specify the versions that Skypilot installs?</p>","answerHtml":"<p>You can create a new conda environment and start the Ray cluster head using the new conda environment. Then, all the workers can connect to the cluster using the new environment as well. Here's an example snippet:</p>\n<pre><code class=\"hljs language-yaml\"><span class=\"hljs-attr\">setup:</span> <span class=\"hljs-string\">|\n  conda activate myray\n  if [ $? != \"0\" ]; then\n    conda create -n myray -y python=${PYTHON_VERSION}\n    conda activate myray\n  fi\n  pip install &#x3C;https://s3-us-west-2.amazonaws.com/ray-wheels/master/${RAY_COMMIT}/ray-3.0.0.dev0-cp38-cp38-manylinux2014_x86_64.whl>\n</span>\n  <span class=\"hljs-comment\"># works for single node (if you need to do multiple nodes, we can talk more about it)</span>\n  <span class=\"hljs-string\">ray</span> <span class=\"hljs-string\">start</span> <span class=\"hljs-string\">--head</span>\n\n<span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n  # you should be able to connect to your own ray cluster in your program\n  conda activate myray\n  python myprogram.py\n</span></code></pre>"}
{"title":"Why is the 'ports' field unsupported in my YAML file when creating a cluster using SkyPilot?","descriptionHtml":null,"answerHtml":"<p>The 'ports' field is currently unsupported in the version you are using. To get this feature, you can install SkyPilot from source or use the nightly build. After launching the cluster on AWS, the specified ports will be accessible outside the public cloud, and inbound rules will be automatically added to the security group used by the instance.</p>"}
{"title":"Is storage required for using SkyServe with Azure?","descriptionHtml":null,"answerHtml":"<p>Yes, currently SkyServe requires a storage cloud to be enabled. However, Azure Blob Storage is not supported yet. You can try using an AWS or GCP trial account to enable S3 or GCS.</p>"}
{"title":"How to install Sky Serve in Skypilot?","descriptionHtml":null,"answerHtml":"<p>To install Sky Serve in Skypilot, you can either install the latest nightly release using the command <code>pip install -U \"skypilot-nightly[all]\"</code>, or install from source by cloning the Skypilot repository and running <code>pip install \".[all]\"</code> in the cloned directory.</p>"}
{"title":"How does the optimizer work to get the lowest VM price across clouds?","descriptionHtml":null,"answerHtml":"<p>The optimizer queries the service catalog, which contains instance types, their resources, and prices across clouds. It then runs an ILP (Integer Linear Programming) to minimize cost. If you want more details, you can refer to the paper mentioned in the discussion thread: <a href=\"https://www.usenix.org/system/files/nsdi23-yang-zongheng.pdf\" rel=\"nofollow\" target=\"_blank\">Optimizer Paper</a>. Additionally, if you have specific questions about the code, you can ask for specific places to refer to.</p>"}
{"title":"What is the difference between having `use_spot: True` in the yaml and running `sky spot launch` ?","descriptionHtml":null,"answerHtml":"<p>The difference is that having <code>use_spot: True</code> in the yaml file launches an unmanaged spot task, which does not automatically recover from preemptions. On the other hand, running <code>sky spot launch</code> or <code>sky.spot_launch()</code> launches a managed spot task, which enables auto-recovery.</p>"}
{"title":"How can I access the disk_size field from node_provider.create_node() in skypilot?","descriptionHtml":"<p>I am writing a node provider and I need to access the disk_size field from node_provider.create_node(). However, the node_config Dict only has the instance type in it. How can I get skypilot to pass in more info from the resources?</p>","answerHtml":"<p>To access the disk_size field from node_provider.create_node() in skypilot, you can refer to this PR: <a href=\"https://github.com/skypilot-org/skypilot/commit/a1b0bd309dc5d047f24dca4c94659d8b716f918b#diff-73daa838267cbf9c1f6491ea3898538cf07a4f9c3993103d6f7ae6d3fc4dc721R49\" rel=\"nofollow\" target=\"_blank\">link</a>. It shows an example of how to pass additional information from the resources using the provisioner API. Additionally, you may find this PR helpful for adding a new cloud using the new provisioner API: <a href=\"https://github.com/skypilot-org/skypilot/pull/2880\" rel=\"nofollow\" target=\"_blank\">link</a>.</p>"}
{"title":"What might be causing the 'RuntimeError: dictionary changed size during iteration' error?","descriptionHtml":null,"answerHtml":"<p>The 'RuntimeError: dictionary changed size during iteration' error is caused by a bug that was introduced recently. A hot fix has been sent to address this issue. You can find the fix in this pull request: <a href=\"https://github.com/skypilot-org/skypilot/pull/2876\" rel=\"nofollow\" target=\"_blank\">link</a>.</p>"}
{"title":"Why did my spot job launch in AWS instead of GCP?","descriptionHtml":null,"answerHtml":"<p>The spot controller is launched in AWS by default, and the spot job itself should adhere to the optimizer table. The optimizer determined that AWS instance/location is cheaper than any other choice in your search space. If you want to change the spot controller location, you can configure it in the global config file. The intention of this design is that the spot controller is a global, long-lived VM shared among all spot jobs.</p>"}
{"title":"Why the need for continuous sync rather than let the next sky launch/exec handle it?","descriptionHtml":"<p>Understanding the purpose of continuous sync in skypilot</p>","answerHtml":"<p>The need for continuous sync arises when there is a requirement to sync project code and run commands remotely with ssh. Continuous sync allows for real-time updates and propagation of changes to the code on the cluster. It can be useful in workflows like a jupyter + nbdev setup, where changes need to be propagated to code in other notebooks/files on the cluster. The use of <code>sky exec</code> instead of manually running commands with ssh can also be considered as an alternative workflow.</p>"}
{"title":"How can I define custom VPC and images in Skypilot to suggest cost-effective cloud options?","descriptionHtml":null,"answerHtml":"<p>To define custom VPC and images in Skypilot, you can use the <code>resources</code> section in the task.yaml file. You can specify multiple resources using the <code>any_of</code> keyword and define the cloud provider and image ID for each resource. Here's an example:</p>\n<pre><code class=\"hljs language-yaml\"><span class=\"hljs-attr\">resources:</span>\n  <span class=\"hljs-attr\">any_of:</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">cloud:</span> <span class=\"hljs-string\">aws</span>\n      <span class=\"hljs-attr\">image_id:</span>\n        <span class=\"hljs-attr\">us-east-1:</span> <span class=\"hljs-string\">ami-xxxx</span>\n        <span class=\"hljs-attr\">us-east-2:</span> <span class=\"hljs-string\">ami-yyyy</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">cloud:</span> <span class=\"hljs-string\">gcp</span>\n      <span class=\"hljs-attr\">image_id:</span> <span class=\"hljs-string\">projects/xxx</span></code></pre>\n<p>This allows Skypilot to suggest cost-effective cloud options and pick the custom image and VPC based on the specified criteria.</p>"}
{"title":"How can I specify specific resources for my job to run in any US region?","descriptionHtml":null,"answerHtml":"<p>You can specify the specific resources for your job to run in any US region by using the <code>any_of</code> syntax. Here is an example:</p>\n<pre><code class=\"hljs language-resources:\">  accelerators: T4:2\n  cloud: gcp\n  any_of:\n    - region: us-east5\n    - region: us-west1```</code></pre>"}
{"title":"How can you access Ray dashboard for a specific SkyServe head node if service replicas = 2?","descriptionHtml":null,"answerHtml":"<p>Currently, accessing the Ray dashboard for a specific SkyServe head node when service replicas are set to 2 is not supported. However, there is an open issue on GitHub to track this feature request: <a href=\"https://github.com/skypilot-org/skypilot/issues/2859\" rel=\"nofollow\" target=\"_blank\">link</a></p>"}
{"title":"Does sky pilot serve support Ray serve composite apps where >1 serve deployment are assembled in a pipeline?","descriptionHtml":"<p>How to work around a pipeline that may require Ray serve > the default Ray 2.4 that sky pilot provisions on a service node</p>","answerHtml":"<p>Yes, the dependency conflict can be fixed by using the PR mentioned in the discussion thread. You can find more details in the provided link.</p>"}
{"title":"How can I move a stopped spot instance to a new zone in GCP?","descriptionHtml":"<p>Is it possible to move a stopped spot instance to a new zone in GCP instead of recreating it?</p>","answerHtml":"<p>Moving a stopped spot instance to a new zone in GCP is not officially supported in skypilot. However, you can use the <code>gcloud</code> command-line tool to manually move the instance to a new zone. Here is an example command:</p>\n<pre><code class=\"hljs language-bash\">gcloud compute instances move INSTANCE_NAME --zone=NEW_ZONE</code></pre>"}
{"title":"How to debug a skypilot job stuck in 'pending' state?","descriptionHtml":"<p>What steps can be taken to debug a skypilot job that is stuck in the 'pending' state?</p>","answerHtml":"<p>To debug a skypilot job that is stuck in the 'pending' state, you can follow these steps:</p>\n<ol>\n<li>Check the job queue using the command <code>sky queue jupyter</code> to see the status of the job.</li>\n<li>Check the logs of the job using the command <code>tail -f ~/sky_logs/sky-2023-12-08-15-56-32-623003/*</code> to look for any error messages or issues.</li>\n<li>Run <code>ray job list</code> on the remote cluster to see the current status of the job.</li>\n<li>Share the task yaml file with the skypilot team to help them reproduce the issue.</li>\n<li>If the job was cloned from an existing cluster, it could be caused by stalled metadata on the remote cluster. In this case, the skypilot team can investigate and resolve the issue.</li>\n</ol>\n<p>It is also recommended to provide any relevant information or error messages when seeking help with debugging a skypilot job.</p>"}
{"title":"How can I declaratively specify port ranges on the 'serve controller' in SkyServe?","descriptionHtml":"<p>I noticed that the examples do not specify port ranges and create a default range. Can I customize the port ranges?</p>","answerHtml":"<p>Currently, it doesn't support custom port ranges for the controller. The load balancing is done using a FastAPI running on your controller VM, which uses HTTP redirect to distribute traffic to replicas. However, there is an open issue <a href=\"https://github.com/skypilot-org/skypilot/issues/2853\" rel=\"nofollow\" target=\"_blank\">https://github.com/skypilot-org/skypilot/issues/2853</a> to keep track of this requirement.</p>"}
{"title":"How to fix a connection refused error when running Ray?","descriptionHtml":null,"answerHtml":"<p>To fix a connection refused error when running Ray, you can try the following steps:</p>\n<ol>\n<li>Relaunch the cluster using <code>sky launch -c cluster-name</code> to restore any defunct runtime.</li>\n<li>SSH into the cluster and check if the Ray dashboard is up using <code>ray status</code> or <code>ps aux | grep -i ray</code>.</li>\n<li>If the above steps don't work, try bringing up a new cluster and see if the error persists.</li>\n</ol>"}
{"title":"How can I overcome the permission issue when mounting an R2 to a mount point using sudo?","descriptionHtml":"<p>I am getting a permission error when trying to mount an R2 to a mount point /tools using sudo. How can I resolve this?</p>","answerHtml":"<p>To overcome the permission issue when mounting an R2 to a mount point using sudo, you can try adding <code>sudo</code> to the source code where the mounting command is constructed. Specifically, you can modify the <code>sky.data.storage.R2Store.mount_command()</code> function. However, please note that this may change the permission requirements for the <code>/tools</code> directory, as it will be mounted with sudo. Additionally, it is worth considering whether there is a specific need to set the directory at <code>/tools</code> instead of <code>/home/ubuntu/tools</code>.</p>"}
{"title":"Why does `sky stop` not work on GCP spot instances?","descriptionHtml":"<p>Is there a reason why <code>sky stop</code> is not allowed on GCP spot instances?</p>","answerHtml":"<p>The decision to not allow <code>sky stop</code> on GCP spot instances was made based on the behavior of AWS. AWS throws an error when trying to stop a spot instance associated with a one-time spot instance request. However, on GCP, the disks attached to spot instances are preserved and can be accessed and recovered. This behavior is different from AWS. The issue has been filed to track this improvement.</p>"}
{"title":"Can I choose a specific OS as a resource in SkyPilot?","descriptionHtml":"<p>Is it possible to choose CentOS 7 as the OS instead of the default Ubuntu?</p>","answerHtml":"<p>Yes, it is possible to choose a specific OS as a resource in SkyPilot. By default, SkyPilot uses Ubuntu as the OS. However, if you want to use CentOS 7, you can choose a custom CentOS 7 AMI. When launching your job, make sure to have the necessary tools installed in the CentOS AMI (e.g., wget, conda, pip3, sudo permission). You don't need to create the user 'ubuntu' in your CentOS AMI.</p>"}
{"title":"Is it possible to use francecentral Azure region in Sky pilot?","descriptionHtml":"<p>Is francecentral Azure region supported in Sky pilot?</p>","answerHtml":"<p>Yes, francecentral Azure region is supported in Sky pilot. However, the user was experiencing an error because they were running the python command in the wrong folder. The command should be run inside the skypilot installation folder.</p>"}
{"title":"What is the recommended way to run SkyPilot on Windows?","descriptionHtml":null,"answerHtml":"<p>The recommended way to run SkyPilot on Windows is through WSL (Windows Subsystem for Linux). Windows support is tracked in this GitHub issue: <a href=\"https://github.com/skypilot-org/skypilot/issues/1981\" rel=\"nofollow\" target=\"_blank\">skypilot-org/skypilot/issues/1981</a></p>"}
{"title":"Is there any optimizer other than minimizing cost, such as latency and performance? If so, where can I choose a different optimizer?","descriptionHtml":null,"answerHtml":"<p>Currently, only cost optimization is supported. However, there are plans for a carbon-aware optimizer that aims to pick the 'greenest' VM for a specified job. To optimize for latency, you can manually specify regions in the job YAML file. For performance, VMs with the same resource specification are expected to have similar raw performance. Network latency should not impact simulation performance if the simulation does not need to communicate with your laptop. The size of the design spec file and the simulation duration will determine the impact of network latency during the upload.</p>"}
{"title":"Is the ability to run a DAG now supported by skypilot?","descriptionHtml":null,"answerHtml":"<p>Yes, skypilot now supports running DAGs using spot pipelines. You can use the <code>spot launch</code> command on a sequential chain of tasks. The topography of the tasks can be controlled by writing them in the desired order in the YAML file. Additionally, you can write logic to launch tasks on the cluster and control the topography by waiting for task completion or creating an orchestrator task that launches other tasks within the cluster.</p>"}
{"title":"Is there a way to retain the logs after the job is completed?","descriptionHtml":null,"answerHtml":"<p>Yes, you can copy over the logs to a mounted object store or use <code>aws s3 sync</code> to sync the logs to an S3 bucket. Here's an example:</p>\n<pre><code class=\"hljs language-yaml\"><span class=\"hljs-attr\">file_mounts:</span>\n  <span class=\"hljs-string\">/mylogs:</span>\n    <span class=\"hljs-attr\">source:</span> <span class=\"hljs-string\">'&#x3C;s3://romil-logs>'</span>\n    <span class=\"hljs-attr\">mode:</span> <span class=\"hljs-string\">MOUNT</span>\n\n<span class=\"hljs-attr\">setup:</span> <span class=\"hljs-string\">|\n  echo Setup\n</span>\n<span class=\"hljs-attr\">run:</span> <span class=\"hljs-string\">|\n  echo MyRun\n  rsync -avz ~/sky_logs /mylogs\n</span></code></pre>\n<p>With this, the logs will be stored in the mounted object store or the specified S3 bucket.</p>"}
{"title":"Why is sky status not showing clusters and jobs running after installing sky cli on a new machine?","descriptionHtml":"<p>Do I need to do something else to make this work?</p>","answerHtml":"<p>There could be several reasons why sky status is not showing clusters and jobs running after installing sky cli on a new machine. One possibility is that the AWS account configuration is not properly set up on the new machine. Another possibility is that there may be an issue with the installation or configuration of the sky cli itself. It's also worth checking if there are any network connectivity issues between the new machine and the target machine where the jobs are supposed to run. To troubleshoot the issue, you can try the following steps:</p>\n<ol>\n<li>Double-check the AWS account configuration on the new machine and make sure it matches the one used on the previous machine.</li>\n<li>Verify that the sky cli is correctly installed and configured on the new machine.</li>\n<li>Check for any error messages or logs that could indicate the cause of the issue.</li>\n<li>Test the network connectivity between the new machine and the target machine to ensure there are no connectivity issues.</li>\n</ol>\n<p>If the issue persists, it may be helpful to reach out to the skypilot community or support for further assistance.</p>"}
{"title":"Is there a way to specify multiple regions?","descriptionHtml":"<p>Like if I want to run spot instance only in US regions?</p>","answerHtml":"<p>Yes, you can specify multiple regions when running spot instances. You can use the <code>--region</code> flag in the AWS CLI or specify the region in your code when using the AWS SDK. Here's an example using the AWS CLI:</p>\n<pre><code>aws ec2 run-instances --region us-west-1,us-west-2 --instance-type t2.micro --image-id ami-1234567890abcdef0</code></pre>"}
{"title":"How to set up a multi-node on k8s setup using SkyPilot?","descriptionHtml":null,"answerHtml":"<p>To set up a multi-node on k8s setup using SkyPilot, you can use the <code>num_nodes</code> parameter in the <code>sky.Task</code> object. Make sure you are using SkyPilot version 0.4.1 or above. Here is an example script:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> sky\n\ntask = sky.Task(\n    run=<span class=\"hljs-string\">'echo $SKYPILOT_NODE_IPS'</span>,\n    num_nodes=<span class=\"hljs-number\">2</span>\n)\nsky.launch(\n    task,\n    cluster_name=<span class=\"hljs-string\">'mycluster'</span>,\n)</code></pre>\n<p>This script will spin up a cluster with 2 nodes and run the command <code>echo $SKYPILOT_NODE_IPS</code> on each node.</p>"}
{"title":"How can I resolve the issue with skypilot installing awscli and clobbering the aws command in a virtualenv?","descriptionHtml":null,"answerHtml":"<p>One workaround for this issue is to create an alias for the aws command, such as <code>aws2</code>, and use that for anything SSO related. This can be done by running the command <code>alias aws2=/usr/local/bin/aws</code>. However, this workaround may not be ideal for everyone as it requires setting up the alias for each user. Another option is to manually remove the [aws] extras from the skypilot installation, but this may not remove the awscli dependency. It's important to note that this issue occurs when skypilot is installed and overrides the existing aws command with aws cli v1, even if aws cli v2 was already installed.</p>"}
{"title":"How can I upload files from a local directory to an existing S3 bucket using the Python API?","descriptionHtml":null,"answerHtml":"<p>To upload files from a local directory to an existing S3 bucket using the Python API, you can use the <code>sky</code> library. However, note that the library currently does not support mounting a specific sub-path on a bucket. The root of the bucket is mounted at the specified mount point. To achieve your goal, you can change the path where you write your checkpoints such that it is relative to the root of the mount point. Here's an example snippet:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> contextlib\n<span class=\"hljs-keyword\">import</span> sky\n\ncluster = <span class=\"hljs-string\">\"my-cluster\"</span>\n\n<span class=\"hljs-keyword\">with</span> contextlib.suppress(ValueError):\n    sky.down(cluster)\n\n<span class=\"hljs-comment\"># Create a task</span>\n\n<span class=\"hljs-comment\"># Set resources</span>\n\n<span class=\"hljs-comment\"># Set storage mounts</span>\n\n<span class=\"hljs-comment\"># Launch the task</span>\n</code></pre>"}
{"title":"Is the `workdir` in the yaml file relative to the user's current working directory or relative to the yaml file itself?","descriptionHtml":null,"answerHtml":"<p>Yes, the <code>workdir</code> specified in the yaml file is relative to the user's current working directory. This behavior is similar to python or bash scripts, where the relative path in the file is relative to the place where the command is run, rather than relative to the script/yaml file itself.</p>"}
{"title":"Is there a way to configure managed spot instances to automatically shut down upon launch?","descriptionHtml":null,"answerHtml":"<p>Yes, for managed spot instances, they are automatically shut down upon job exit. To launch a managed spot instance and have it shut down after executing a command, you can use the <code>sky spot launch</code> command followed by the command you want to execute. For example:</p>\n<pre><code>sky spot launch -- echo hi</code></pre>\n<p>This will launch the spot instance, execute the command <code>echo hi</code>, and then shut down the instance.</p>"}
{"title":"Can the Ray dashboard be enabled in SkyPilot for observability purposes?","descriptionHtml":null,"answerHtml":"<p>Yes, the Ray dashboard can be enabled in SkyPilot for observability purposes. By default, it should be running on the head node. To enable it, you can use the following code snippet:</p>\n<pre><code class=\"hljs language-RAY_DASHBOARD_PORT=&#x22;${RAY_DASHBOARD_PORT:-8266}&#x22;\">\nremote_host=$(sky status | awk 'NR==3{print $1}')\nssh -fNT -L \"${RAY_DASHBOARD_PORT}:localhost:${RAY_DASHBOARD_PORT}\" \"$remote_host\" #&#x26;gt;/dev/null 2&#x26;gt;&#x26;amp;1```</code></pre>"}
{"title":"How to debug a skypilot job stuck in 'pending' state?","descriptionHtml":"<p>What steps can be taken to debug a skypilot job that is stuck in the 'pending' state?</p>","answerHtml":"<p>To debug a skypilot job that is stuck in the 'pending' state, you can follow these steps:</p>\n<ol>\n<li>Check the job queue using the command <code>sky queue jupyter</code> to see the status of the job.</li>\n<li>Check the logs of the job using the command <code>tail -f ~/sky_logs/sky-2023-12-08-15-56-32-623003/*</code> to look for any error messages or issues.</li>\n<li>Run <code>ray job list</code> on the remote cluster to see the current status of the job.</li>\n<li>Share the task yaml file with the skypilot team to help them reproduce the issue.</li>\n<li>If the job was cloned from an existing cluster, it could be caused by stalled metadata on the remote cluster. In this case, the skypilot team can investigate and resolve the issue.</li>\n</ol>\n<p>It is also recommended to provide any relevant information or error messages when seeking help with debugging a skypilot job.</p>"}
{"title":"How can I move a stopped spot instance to a new zone in GCP?","descriptionHtml":"<p>Is it possible to move a stopped spot instance to a new zone in GCP instead of recreating it?</p>","answerHtml":"<p>Moving a stopped spot instance to a new zone in GCP is not officially supported in skypilot. However, you can use the <code>gcloud</code> command-line tool to manually move the instance to a new zone. Here is an example command:</p>\n<pre><code class=\"hljs language-bash\">gcloud compute instances move INSTANCE_NAME --zone=NEW_ZONE</code></pre>"}
{"title":"Does sky pilot serve support Ray serve composite apps where >1 serve deployment are assembled in a pipeline?","descriptionHtml":"<p>How to work around a pipeline that may require Ray serve > the default Ray 2.4 that sky pilot provisions on a service node</p>","answerHtml":"<p>Yes, the dependency conflict can be fixed by using the PR mentioned in the discussion thread. You can find more details in the provided link.</p>"}
{"title":"How can you access Ray dashboard for a specific SkyServe head node if service replicas = 2?","descriptionHtml":null,"answerHtml":"<p>Currently, accessing the Ray dashboard for a specific SkyServe head node when service replicas are set to 2 is not supported. However, there is an open issue on GitHub to track this feature request: <a href=\"https://github.com/skypilot-org/skypilot/issues/2859\" rel=\"nofollow\" target=\"_blank\">link</a></p>"}
{"title":"How can I specify specific resources for my job to run in any US region?","descriptionHtml":null,"answerHtml":"<p>You can specify the specific resources for your job to run in any US region by using the <code>any_of</code> syntax. Here is an example:</p>\n<pre><code class=\"hljs language-resources:\">  accelerators: T4:2\n  cloud: gcp\n  any_of:\n    - region: us-east5\n    - region: us-west1```</code></pre>"}
{"title":"How can I define custom VPC and images in Skypilot to suggest cost-effective cloud options?","descriptionHtml":null,"answerHtml":"<p>To define custom VPC and images in Skypilot, you can use the <code>resources</code> section in the task.yaml file. You can specify multiple resources using the <code>any_of</code> keyword and define the cloud provider and image ID for each resource. Here's an example:</p>\n<pre><code class=\"hljs language-yaml\"><span class=\"hljs-attr\">resources:</span>\n  <span class=\"hljs-attr\">any_of:</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">cloud:</span> <span class=\"hljs-string\">aws</span>\n      <span class=\"hljs-attr\">image_id:</span>\n        <span class=\"hljs-attr\">us-east-1:</span> <span class=\"hljs-string\">ami-xxxx</span>\n        <span class=\"hljs-attr\">us-east-2:</span> <span class=\"hljs-string\">ami-yyyy</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">cloud:</span> <span class=\"hljs-string\">gcp</span>\n      <span class=\"hljs-attr\">image_id:</span> <span class=\"hljs-string\">projects/xxx</span></code></pre>\n<p>This allows Skypilot to suggest cost-effective cloud options and pick the custom image and VPC based on the specified criteria.</p>"}
{"title":"Why the need for continuous sync rather than let the next sky launch/exec handle it?","descriptionHtml":"<p>Understanding the purpose of continuous sync in skypilot</p>","answerHtml":"<p>The need for continuous sync arises when there is a requirement to sync project code and run commands remotely with ssh. Continuous sync allows for real-time updates and propagation of changes to the code on the cluster. It can be useful in workflows like a jupyter + nbdev setup, where changes need to be propagated to code in other notebooks/files on the cluster. The use of <code>sky exec</code> instead of manually running commands with ssh can also be considered as an alternative workflow.</p>"}
{"title":"How does the optimizer work to get the lowest VM price across clouds?","descriptionHtml":null,"answerHtml":"<p>The optimizer queries the service catalog, which contains instance types, their resources, and prices across clouds. It then runs an ILP (Integer Linear Programming) to minimize cost. If you want more details, you can refer to the paper mentioned in the discussion thread: <a href=\"https://www.usenix.org/system/files/nsdi23-yang-zongheng.pdf\" rel=\"nofollow\" target=\"_blank\">Optimizer Paper</a>. Additionally, if you have specific questions about the code, you can ask for specific places to refer to.</p>"}
{"title":"Why did my spot job launch in AWS instead of GCP?","descriptionHtml":null,"answerHtml":"<p>The spot controller is launched in AWS by default, and the spot job itself should adhere to the optimizer table. The optimizer determined that AWS instance/location is cheaper than any other choice in your search space. If you want to change the spot controller location, you can configure it in the global config file. The intention of this design is that the spot controller is a global, long-lived VM shared among all spot jobs.</p>"}
{"title":"What might be causing the 'RuntimeError: dictionary changed size during iteration' error?","descriptionHtml":null,"answerHtml":"<p>The 'RuntimeError: dictionary changed size during iteration' error is caused by a bug that was introduced recently. A hot fix has been sent to address this issue. You can find the fix in this pull request: <a href=\"https://github.com/skypilot-org/skypilot/pull/2876\" rel=\"nofollow\" target=\"_blank\">link</a>.</p>"}
{"title":"How to Resolve SkyPilot's INIT Status and FetchIPError After Internet Disconnection?","descriptionHtml":"<p>I was relaunching a stopped cluster using SkyPilot when my internet connection was interrupted. Now, SkyPilot seems to be stuck in limbo, and the cluster status remains 'INIT' even after 20 minutes. I've tried to <code>sky launch</code> multiple times, but I keep encountering a <code>sky.exceptions.FetchIPError</code>. Here's the command I used and the error output:</p>\n<pre><code class=\"hljs language-bash\">$ sky launch -c spot-jupyter --use-spot jupyter.yaml\n<span class=\"hljs-comment\"># ... error output ...</span>\nsubprocess.CalledProcessError: Command <span class=\"hljs-string\">'ray get-head-ip '</span>/var/folders/z1/yww43t6d0vl6qp45379h2n540000gn/T/tmpdu3ug4w9<span class=\"hljs-string\">''</span> returned non-zero <span class=\"hljs-built_in\">exit</span> status 1.\n<span class=\"hljs-comment\"># ... more error output ...</span>\nsky.exceptions.FetchIPError</code></pre>\n<p>How can I resolve this issue?</p>","answerHtml":"<p>Zhanghao Wu from SkyPilot suggested that I try <code>sky launch</code> again on the cluster, as it often fixes the <code>INIT</code> status. However, when I attempted this, it resulted in the same IPError each time. Zhanghao then inquired if I had checked the GCP console for preemption issues, which I had not. He also mentioned that a more robust GCP provisioner is being developed for SkyPilot, which should address many current issues, including the one I'm facing. Additionally, Zongheng Yang noted that stopping GCP spot instances has been added in a recent pull request under review.</p>"}
{"title":"How can I access the disk_size field from node_provider.create_node() in skypilot?","descriptionHtml":"<p>I am writing a node provider and I need to access the disk_size field from node_provider.create_node(). However, the node_config Dict only has the instance type in it. How can I get skypilot to pass in more info from the resources?</p>","answerHtml":"<p>To access the disk_size field from node_provider.create_node() in skypilot, you can refer to this PR: <a href=\"https://github.com/skypilot-org/skypilot/commit/a1b0bd309dc5d047f24dca4c94659d8b716f918b#diff-73daa838267cbf9c1f6491ea3898538cf07a4f9c3993103d6f7ae6d3fc4dc721R49\" rel=\"nofollow\" target=\"_blank\">link</a>. It shows an example of how to pass additional information from the resources using the provisioner API. Additionally, you may find this PR helpful for adding a new cloud using the new provisioner API: <a href=\"https://github.com/skypilot-org/skypilot/pull/2880\" rel=\"nofollow\" target=\"_blank\">link</a>.</p>"}
{"title":"Is storage required for using SkyServe with Azure?","descriptionHtml":null,"answerHtml":"<p>Yes, currently SkyServe requires a storage cloud to be enabled. However, Azure Blob Storage is not supported yet. You can try using an AWS or GCP trial account to enable S3 or GCS.</p>"}
{"title":"Are there plans to align Skypilot with the main Ray project?","descriptionHtml":"<p>Can Skypilot align with the main Ray project to make sky serve and Ray serve more interoperable?</p>","answerHtml":"<p>Yes, there are plans to align Skypilot with the main Ray project. This will make sky serve and Ray serve more interoperable. By aligning with the main Ray project, Skypilot can leverage the latest features and improvements from Ray, ensuring compatibility and enhancing interoperability between the two projects. This alignment will benefit users who use both Skypilot and Ray serve, enabling them to seamlessly integrate and leverage the functionalities of both platforms.</p>"}
{"title":"Has Skypilot considered integrating with SLURM?","descriptionHtml":"<p>I noticed that the job scheduler in Skypilot is similar to the <code>srun</code> CLI used in SLURM. Has the Skypilot team considered integrating with SLURM?</p>","answerHtml":"<p>Yes, the Skypilot team has considered integrating with SLURM. However, they have prioritized supporting Kubernetes for on-prem deployments due to its popularity and comprehensive feature set. They are open to contributions adding support for SLURM. SLURM is known for its ability to share finite resources in an on-prem HPC cluster, while Kubernetes is designed for dedicated usage of resources provisioned from an 'infinite' pool.</p>"}
{"title":"Are there any plans to implement the ability to choose Custom V-net and custom images in Azure?","descriptionHtml":"<p>The user is asking if there are any plans to implement the ability to choose Custom V-net and custom images in Azure.</p>","answerHtml":"<p>Yes, there are plans to implement the ability to choose Custom V-net and custom images in Azure. However, it is currently not available. You can check for updates on the Azure roadmap or reach out to the Azure support team for more information.</p>"}
{"title":"How long does it typically take to provision the Vicuna model following the Sky serve tutorial?","descriptionHtml":"<p>I'm trying to provision the Vicuna model following the Sky serve tutorial, but it has been running for about an hour and a half. I have successfully run the 'Hello, SkyServe!' example. Any idea how long it usually takes to provision the model?</p>","answerHtml":"<p>The provisioning time for the Vicuna model can vary depending on various factors such as the availability of resources and the chosen accelerator. However, it is not uncommon for the provisioning process to take more than an hour. If the provisioning process is taking longer than expected, it is recommended to check the logs for any errors or issues. Additionally, it may be worth considering using a different accelerator, such as V100 or spot A100, if the OnDemand A100 is scarce.</p>"}
