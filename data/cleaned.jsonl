{"title": "Is there a debugging guide / TRACE / detailed debugging one can get?", "description": "I'm trying to figure out why my exec jobs are hanging on Azure. My setup/run\nworks fine on GCP, but I need more detailed logs to debug the issue.", "answer": "For user job logs, you can check `sky logs <cluster-name> <job-id>`.\nAdditionally, you can run `ray job list` on the VM to help debug the issue. It\nseems that the python version on your Azure cluster is Python 2.7.18, which\nmight be causing the problem. You can try upgrading to the latest version of\nSkyPilot to see if that solves the issue."}
{"title": "How to manage skypilot SSH keys and cloud credentials for CI/Github actions?", "description": "", "answer": "To manage skypilot SSH keys and cloud credentials for CI/Github actions, you\ncan follow these steps:\n\n  1. For cloud credentials, determine which cloud provider you are using (e.g., GCP, AWS, Azure).\n  2. Set up the necessary environment variables or configuration files for the cloud provider in your CI/Github actions workflow.\n  3. For skypilot SSH keys, you can create and store them securely in your CI/Github actions secrets or encrypted files.\n  4. In your deployment workflow, use the appropriate commands or tools to configure the SSH keys and cloud credentials for skypilot.\n\nHere's an example for GCP:\n\n    \n    \n    -v \"$HOME/.ssh/sky-key.pub:/root/.ssh/sky-key.pub\" \\\n    -v \"$HOME/.ssh/sky-key:/root/.ssh/sky-key\" \\\n    -v \"$HOME/.sky:/root/.sky\" ```"}
{"title": "Encountering cluster provisioning issue with Skypilot and Kubernetes", "description": "", "answer": "It seems that the issue is related to the kubelet not running or being\nunhealthy. The error message suggests checking the kubelet status using the\n`systemctl status kubelet` command and inspecting its logs using `journalctl\n-xeu kubelet`. Additionally, it's recommended to check for any crashed or\nexited control plane components and list all running Kubernetes containers\nusing the `crictl` command. The user who reported the issue mentioned using\nAWS EC2 instance with AWS Linux. Another user tried to replicate the issue but\nwas unable to do so on a similar environment. They shared their command logs\nand suggested running `export SKYPILOT_DEBUG=1` before running any `sky`\ncommands for more detailed logs. They also mentioned installing SkyPilot,\ndocker, kubectl, and kind on the machine. It would be helpful for the user to\nshare their command line logs after spinning up a VM to identify the specific\ncommand that fails."}
{"title": "Does the YAML allow for registering multiple models?", "description": "Is this the idea behind the multiple nodes approach?", "answer": "Yes, you can register multiple models in YAML. The multiple nodes approach\nallows you to run multiple models on a single VM. You can either use multiple\nYAML files, one for each model, or use a single YAML file and use `&` to put\neach model worker to background in the `run` commands."}
{"title": "What is the issue with running a task using SkyPilot on a local cluster and how to solve it?", "description": "", "answer": "The issue is that the 'cloud', 'region', and 'instance_type' parameters must\nbe set by the optimizer. To solve this, you can follow the recommended way of\nutilizing on-prem resources by installing Kubernetes on your on-prem cluster\nand using SkyPilot's Kubernetes support to submit tasks. If you're on GCP, you\ncan refer to the provided guide to set up a Kubernetes cluster on GCP VMs and\nsubmit tasks to it using SkyPilot. Note that GPU support is currently in-\nprogress and only single-node CPU jobs are supported in the master branch."}
{"title": "Can I use skypilot On-Prem to work with a VM acquired from a cloud not supported by skypilot?", "description": "", "answer": "Yes, you can use skypilot On-Prem to work with a VM acquired from a cloud not\nsupported by skypilot. However, the current On-Prem support has been\ndeprecated and is no longer supported. Instead, it is recommended to use the\nnew and experimental Kubernetes support. This will allow you to run SkyPilot\ntasks on a Kubernetes cluster. To try it out, you will need to deploy a\nKubernetes cluster on your VM and follow the provided steps. Note that this\nfeature is under active development and some functionalities may not be\nsupported yet."}
{"title": "What could be causing the performance issues when launching with Skypilot?", "description": "The user is experiencing performance issues when launching with Skypilot and\nwants to know the possible causes.", "answer": "The performance issues could be related to I/O issues on the user's side,\nspecifically with their SSD storage. The user noticed that their SSD storage\nwas running low and after freeing up space, the performance improved."}
{"title": "Do we need to have sudo rights in the provided nodes in order for skypilot to work?", "description": "", "answer": "Yes, sudo rights are required in certain places for SkyPilot to work. Most\nplaces where sudo is used are in cluster setup commands and handling file\nmounts/storage mounting. If you are using a custom image where sudo access is\nnot given, you can try leaving out sudo in the setup commands and see if they\ncan be executed in the image. However, it is recommended to have sudo rights\nfor proper functionality."}
{"title": "How can a data scientist connect to the Ray head node directly from their training script without logging into the EC2 instance?", "description": "", "answer": "Yes, it is possible for a data scientist to connect to the Ray head node\ndirectly from their training script without logging into the EC2 instance.\nThey can use the `sky exec` command followed by the cluster name and the\ndesired command, or they can use the `ssh` command followed by the cluster\nname. For example, `sky exec myclus -- echo hi` or `ssh myclus`."}
{"title": "How to add another kernel to Jupyter running in Skypilot?", "description": "", "answer": "To add another kernel to Jupyter running in Skypilot, you can follow these\nsteps:\n\n  1. Install the `ipykernel` package in your conda environment using `conda install ipykernel`.\n  2. Run the command `python -m ipykernel install --user --name=myenv` to install the kernel.\n\nPlease note that you may need to restart Jupyter after installing the kernel.\nIf you are using the cloud provider's deep learning image, it is recommended\nto start your own Jupyter notebook to have more control over the environment."}
{"title": "How can I obtain a list of all the servers running across different clouds using SkyPilot?", "description": "Is there a way to do this? I'm interested not only in the servers that are\nregistered in the SkyPilot database and have a cluster_name, but in all\nservers associated with my credentials.", "answer": "Obtaining a list of all servers running across different clouds using SkyPilot\nis currently out of the project's scope. However, you can explore other tools\nlike cloud_enum (https://github.com/initstring/cloud_enum) which might help\nyou achieve this. It's important to consider privacy concerns as many\norganizations have non-Sky and Sky resources living in the same account."}
{"title": "", "description": "", "answer": "Great job on the project! Looking forward to reading your blog post."}
{"title": "ValueError: Cloud 'Kubernetes' is not a valid cloud among ['aws', 'azure', 'gcp', 'lambda', 'local', 'ibm', 'scp', 'oci']", "description": "", "answer": "This error is likely caused by branch switching. Running `sky check` again\nshould clear it up."}
{"title": "Why is the 'ports' field unsupported in my YAML file when creating a cluster using SkyPilot?", "description": "", "answer": "The 'ports' field is currently unsupported in the version you are using. To\nget this feature, you can install SkyPilot from source or use the nightly\nbuild. After launching the cluster on AWS, the specified ports will be\naccessible outside the public cloud, and inbound rules will be automatically\nadded to the security group used by the instance."}
{"title": "Why is `sky launch` failing to connect to the launched AWS EC2 instance?", "description": "Any idea why all of a sudden `sky launch` fails to connect to the launched AWS\nEC2 instance?", "answer": "There could be multiple reasons why `sky launch` is failing to connect to the\nlaunched AWS EC2 instance. One possible reason is that there is an issue with\nthe SSH connection. Another reason could be that there is a problem with the\nsecurity group configuration. It is also possible that there is an issue with\nthe Skypilot version being used. Upgrading to the latest version may resolve\nthe issue. Additionally, there could be a problem with the AWS account\npermissions. Checking and ensuring that all necessary permissions are set up\nfor the AWS account is recommended. Finally, it is worth noting that cleaning\nup any dangling resources and reattempting the launch may help resolve the\nissue."}
{"title": "Error when trying to deploy a cluster with SkyPilot", "description": "I am getting a `RuntimeError: Fail to build pip wheel for SkyPilot` error when\ntrying to deploy a cluster with SkyPilot. Has anyone encountered this issue\nbefore?", "answer": "The issue was caused by a bug in the nightly build of SkyPilot. The bug has\nbeen fixed and you can try installing the latest version using `pip install -U\n\"skypilot-nightly>=1.0.0.dev20230901\"`. The bug was related to the naming of\nthe wheel file. You can find more details about the bug and the fix in this\n[PR](https://github.com/skypilot-org/skypilot/pull/2502)."}
{"title": "Error running spot jobs with Skypilot", "description": "", "answer": "The error you encountered is a `TypeError` that occurs when a bytes-like\nobject is required, but a string is provided. This issue has already been\naddressed and fixed in the Skypilot repository. To resolve this, you can\nupdate your Skypilot version to the latest nightly build, which includes the\nfix. Alternatively, you can wait for the next official release, as the fix\nwill be included in it. Upgrading to the nightly build or the next release\nshould resolve the issue and prevent the timeout error you experienced."}
{"title": "How can I specify the Ray/Python version that SkyPilot installs?", "description": "I'm launching my training jobs from a docker container using Skypilot, but it\nseems that the Ray cluster needs to be running the exact same Python/Ray\nversions. How can I specify the versions that Skypilot installs?", "answer": "You can create a new conda environment and start the Ray cluster head using\nthe new conda environment. Then, all the workers can connect to the cluster\nusing the new environment as well. Here's an example snippet:\n\n    \n    \n    setup: |\n      conda activate myray\n      if [ $? != \"0\" ]; then\n        conda create -n myray -y python=${PYTHON_VERSION}\n        conda activate myray\n      fi\n      pip install <https://s3-us-west-2.amazonaws.com/ray-wheels/master/${RAY_COMMIT}/ray-3.0.0.dev0-cp38-cp38-manylinux2014_x86_64.whl>\n    \n      # works for single node (if you need to do multiple nodes, we can talk more about it)\n      ray start --head\n    \n    run: |\n      # you should be able to connect to your own ray cluster in your program\n      conda activate myray\n      python myprogram.py"}
{"title": "", "description": "", "answer": "The team spent a lot of time ensuring that the VM and job state transitions\nare handled correctly and guarded against preemptions. As a result, the\nManaged Spot Job should be reasonably hardened now."}
{"title": "Does Skypilot currently only support the following regions? Can Skypilot support more regions?", "description": "", "answer": "Yes, Skypilot currently supports the following regions: 'centralus, eastus,\neastus2, northcentralus, southcentralus, westcentralus, westus, westus2,\nwestus3'. However, it can support more regions by adding them to the list of\nsupported regions in the code."}
{"title": "How can I quickly get the price of all VM instances across all clouds and regions?", "description": "", "answer": "You can use the `sky show-gpus -a` command to get the pricing of all instances\nacross all clouds. If you're looking for a specific GPU type, you can run `sky\nshow-gpus <gpu-type>`. The command will display the pricing information for\neach instance, including the cloud, instance type, device memory, vCPUs, host\nmemory, hourly price, hourly spot price, and region."}
{"title": "How can I query a SkyPilot node's IP address using the Python SDK?", "description": "", "answer": "You can use the following command to query a SkyPilot node's IP address using\nthe Python SDK:\n\n    \n    \n    import sky\n    ip_address = sky.status('vllm-llama2')[0]['handle'].head_ip"}
{"title": "How can I solve the 'wait_ready timeout exceeded' error when running sky spot launch?", "description": "", "answer": "This error usually occurs due to a networking issue. Here are a few steps you\ncan take to troubleshoot:\n\n  1. Check network connectivity to any AWS IP and ensure that VPN is not required.\n  2. Verify if any networking or VPC settings are set in ~/.sky/config.yaml.\n  3. Try running 'sky launch' instead of 'sky spot launch' to see if it works.\n\nIf the issue persists, you can try the following:\n\n  1. Check if you can SSH into the IP directly using the command 'ssh ubuntu@<IP_ADDR> -i ~/.ssh/sky-key'.\n  2. Update SkyPilot to the latest version (0.4 or skypilot-nightly) and try launching the cluster again.\n\nIf none of these steps resolve the issue, you can try launching an AWS\ninstance manually from the console and see if you can SSH into it. If that\nworks, it could indicate a firewall issue on your local machine.\n\nPlease note that the key should be added directly to the authorized keys in\nthe VM by SkyPilot, rather than using a key pair.\n\nIf you are still facing issues, it is recommended to reach out to the SkyPilot\nsupport team for further assistance."}
{"title": "Why is the binding key not showing up in the console when creating an instance?", "description": "", "answer": "The binding key will not be shown in the console for security reasons.\nInstead, it is directly added to the `~/.ssh/authorized_keys` file on the VM.\nYou can verify if the key is correctly set on the VM by SSHing into the VM\nthrough the AWS console and checking the `~/.ssh/authorized_keys` file."}
{"title": "Is there a development branch for doing multinode batch jobs on k8s?", "description": "", "answer": "Yes, there is a development branch for doing multinode batch jobs on k8s. Some\nchanges that need to be made include creating a headless service for the head\nnode pod, using the service IP on the workers to connect to the head node pod,\nand potentially modifying the scheduling code to use service IPs instead of\npod IPs."}
{"title": "What is the idle policy for the cluster in Skypilot?", "description": "", "answer": "The idle policy for the cluster in Skypilot is based on the completion of the\nlast job in the job queue. Once the last job is completed, the node is\nconsidered idle."}
{"title": "Does skypilot support specifying the VNet when spawning an instance on Azure?", "description": "", "answer": "Currently, skypilot does not support specifying the VNet when spawning an\ninstance on Azure. However, the necessary plumbing to make it work involves\nadding a vpc/vnet field in the node provider configuration and passing it to\nthe underlying networking/instance launch call to Azure."}
{"title": "How do I let skypilot create random bucket names instead of specifying them myself?", "description": "When a spot experiment finished, it deletes the bucket, so if another spot\nexperiment is currently running and needs to access said bucket it will fail\ninstead.", "answer": "If the bucket is for data/code from the local disk, you can specify them\nwithout explicitly giving the bucket name, and SkyPilot will create buckets\nwith random name and delete them after spot job finishes. For the mount mode\nfiles, you can set the bucket name to `$MY_BUCKET_NAME` and pass a random\nbucket name using `sky spot launch --env MY_BUCKET_NAME=my-bucket-$(date\n+%s)`."}
{"title": "Is there a plan for supporting Databricks?", "description": "", "answer": "Yes, there is potential for supporting Databricks. Databricks is good at\nrunning data-intensive jobs, but there is a need for running compute-intensive\ntasks on small/medium-sized datasets. Integrating SkyPilot with Databricks can\nprovide the solution for this. It is already possible to run Ray on\nDatabricks, which opens up the possibility for further integration."}
{"title": "Which cloud provider is the easiest to get A100s in your experience?", "description": "", "answer": "In my experience, GCP (Google Cloud Platform) is the easiest cloud provider to\nget A100 GPUs. They can be found in regions like us-east4/5. AWS training\ninstances offered through SageMaker are also a good option for getting A100\nGPUs on demand."}
{"title": "What is the Skypilot orchestrator in ZenML?", "description": "", "answer": "The Skypilot orchestrator is a new addition to the ZenML framework. It allows\nusers to run ZenML pipelines backed by Skypilot, providing more flexibility\nand access to resources on any cloud. It is expected to become popular in the\nML community."}
{"title": "Recipe or wrapper around SkyPilot for serving LLM inferences at scale?", "description": "", "answer": "Yes, there is a serving layer on SkyPilot that is currently in production use\nfor Chatbot Arena with approximately 500k monthly users. You can find more\ntechnical details and information about becoming a pilot user in this link:\n[Skypilot Slack Discussion](https://skypilot-\norg.slack.com/archives/C03HUKKBQ7R/p1692917046577609)"}
{"title": "Any plans to support runpod through sky?", "description": "", "answer": "Yes, RunPod is currently contributing support through a pull request on\nGitHub. You can find more information and contribute by commenting and +1ing\non the pull request: [link](https://github.com/skypilot-\norg/skypilot/pull/2360)"}
{"title": "How do SSH keys work in skypilot?", "description": "Why is there an SSH key name?", "answer": "When instances are created in skypilot, the skypilot public key `~/.ssh/sky-\nkey.pub` on your local machine is added to `~/.ssh/authorized_keys` on the\nremote machine. The SSH key name is used to identify the key file on the local\nmachine and is specified in the `~/.ssh/config` file. This allows you to\neasily SSH onto the cluster head using just the cluster head name."}
{"title": "Who would be the right people to chat with regarding potential partnership opportunities to run SkyPilot with CoreWeave's GPU labelled on-demand nodes?", "description": "", "answer": "You can reach out to Rahul Talari at CoreWeave to discuss potential\npartnership opportunities for running SkyPilot with their GPU labelled on-\ndemand nodes."}
{"title": "What is the VLLM discord?", "description": "", "answer": "The VLLM discord is a platform for communication and collaboration among VLLM\ncommunity members. It is used to discuss VLLM-related topics, share resources,\nand seek help from other members. It is likely that the weird error you are\nexperiencing can be resolved by reaching out to @Woosuk Kwon on the VLLM\ndiscord."}
{"title": "Is EFS supported for mounts?", "description": "", "answer": "EFS is not natively supported for mounts, but it can be achieved using a few\ncommands. Here is a\n[gist](https://gist.github.com/concretevitamin/eec319cea8b2bac3ac03f23f111c0561)\nthat provides the steps to do it."}
{"title": "What tools can integrate well with Skypilot to achieve ML infrastructure features?", "description": "", "answer": "Skypilot can integrate well with ClearML, WandB, or other ML experiments\ntracking tools. It also supports full reproducibility with tools like DVC.\nCheckpointing is supported using private object storage buckets. Skypilot\nenables distributed training to train models in a shorter time. It also allows\nusers to save costs by using spot instances. For re-triggering old\nexperiments, users can easily rerun a job using the YAML file. Skypilot\ncurrently does not have a GUI."}
{"title": "Issue launching V100 on AWS with Deep Learning AMI GPU PyTorch 2.0.1", "description": "I'm encountering an issue when trying to launch a V100 on AWS with the 'Deep\nLearning AMI GPU PyTorch 2.0.1'. The instance starts momentarily and then is\nimmediately stopped. How can I resolve this?", "answer": "The issue is caused by SkyPilot hard-coding the SSH user as 'ubuntu', while\nthe image expects logins from 'ec2-user'. To resolve this, you can temporarily\nhardcode the 'ec2-user' as the SSH user."}
{"title": "How can I deploy an open-source LLM for my team?", "description": "", "answer": "To deploy an open-source LLM for your team, you can follow the guide provided\nby Tyler Dunn in the first version of the deployment guide. You can find the\nguide at this link: <https://github.com/continuedev/deploy-os-code-llm/>. If\nyou have any feedback or need assistance, you can reach out to Tyler Dunn or\nZhanghao Wu (skypilot)."}
{"title": "Parsing output of 'sky show-gpus --all >>out.txt' to a dataframe", "description": "", "answer": "Yes, you can parse the output of 'sky show-gpus --all >>out.txt' to a\ndataframe. One way to do this is by directly dealing with the CSV files in the\n`~/.sky/catalogs` directory. Alternatively, you can use the `service catalog`\npython API provided by Skypilot to query the CSV files. Here's an example:\n\n    \n    \n    import sky\n    \n    sky.clouds.service_catalog.get_accelerator_hourly_cost(acc_name='A100', acc_count=4, use_spot=False, clouds='gcp')"}
{"title": "How can I reuse an existing EC2 instance for the head instance in managed spot runs?", "description": "Is there a way to reuse one of the EC2 instances already running or specify\nresources for the head instance?", "answer": "For managed spot runs, you can specify the resources requirement of the\ncontroller in `~/.sky/config.yaml` file. You can set the number of CPUs\nrequired for the controller by adding the following configuration:\n\n    \n    \n      controller:\n        resources:\n          cpus: 4```\n    \n    To make the configuration take effect, you need to stop the current controller using `sky down sky-spot-controller-<hash>` and then launch it again using `sky spot launch` command. The controller name can be found in the output of `sky status` command.\n    \n    Additionally, for multiple managed spot jobs, they will reuse the same controller node, so the cost for it is amortized."}
{"title": "AWS deployment timeout issue", "description": "", "answer": "The AWS deployment timeout issue could be caused by an incorrect SSH key\nconfiguration. You can try the SSH command mentioned in the provision.log to\nsee if it hangs. If it does, you may need to check the steps suggested in this\nthread: [AWS Deployment SSH Key Configuration](https://skypilot-\norg.slack.com/archives/C03J2KQQZSS/p1695345105283099?thread_ts=1695276478.957079&cid=C03J2KQQZSS)"}
{"title": "Does Skypilot support non US zones on Azure?", "description": "", "answer": "Yes, Skypilot supports non US zones on Azure. You can refer to the SkyPilot\ndocumentation for more information on how to configure and use Skypilot with\nglobal regions."}
{"title": "Is there a Sky Server example for Mistral 7b?", "description": "", "answer": "Yes, you can find a YAML for deploying Mistral with Sky Serve in this gist:\n[link](https://gist.github.com/infwinston/f1b797b8fc200c0c165d2e9e3bdf057d).\nAdditionally, you can refer to the guide\n[here](https://docs.google.com/document/d/1vVmzLF-\nEkG3Moj-q47DQBGvFipK4PNfkz0V6LyaPstE/edit) for more information. The skyserve\nbranch is going to be merged soon to master."}
{"title": "Recovering spot instance job taking too much time", "description": "Am I doing anything wrong?", "answer": "It would be helpful to check the detailed reasons for provisioning failures in\nthe controller logs. This can be done using the `sky spot logs --controller`\ncommand. It is also recommended to check the optimizer table and failover\nsequence in the logs to identify any potential issues."}
{"title": "Is there a way to set custom SKYPILOT_TASK_ID?", "description": "", "answer": "We currently don’t provide such a way, but you might be able to parse the\nTASK_ID in your bash script or program."}
{"title": "What's the best way to store AWS ECR credentials?", "description": "", "answer": "One way to store AWS ECR credentials is by specifying an executable command to\nthe environment variable, so that the password will be queried for each\nrecovery. This can be done by setting the `SKYPILOT_DOCKER_PASSWORD`\nenvironment variable to `$(aws ecr get-login-password --region us-west-2)`.\nAnother option is to pass the AWS secret key using the `--env` flag, but it\nmay not work when using a Docker image in the `resources` section."}
{"title": "What could be causing the SSH error and termination of the VM in skypilot integration with zenml?", "description": "", "answer": "The SSH error could be the cause of the termination of the VM. In some\nexperimental settings, a similar error was observed. It is recommended to\ncheck the SSH keys, manually run variants of the SSH command, and verify the\ninstance's SSH key in the AWS console."}
{"title": "Can I see my team's clusters and perform actions on them using SkyPilot?", "description": "I want to know if I can view and manage my team's clusters on the same cloud\nsubscription using SkyPilot.", "answer": "Yes, you can see your team's clusters and perform actions on them using\nSkyPilot. In a team setting, full visibility can be obtained through the cloud\nconsole. An admin or team lead user can log into the cloud provider (e.g.,\nAzure) and see everything in the subscription. You can also use the cloud\nconsole to debug issues by SSHing or rsyncing files from your machine to\nsomeone on your team's cluster. If a team member goes on vacation, you can\nstop their cluster using the recommended Autostop feature. SkyPilot supports\ntagging instances, so you can easily filter and find clusters based on tags.\nAdditionally, you can distribute common YAML files using GitHub or other\nversion control systems alongside the project code. This allows team members\nto share and use the same YAML files. If you have any specific requirements or\nneed further assistance, feel free to discuss them."}
{"title": "Is there a debugging guide / TRACE / detailed debugging one can get?", "description": "I'm trying to figure out why my exec jobs are hanging on Azure. My setup/run\nworks fine on GCP, but I need more detailed logs to debug the issue.", "answer": "For user job logs, you can check `sky logs <cluster-name> <job-id>`.\nAdditionally, you can run `ray job list` on the VM to help debug the issue. It\nseems that the python version on your Azure cluster is Python 2.7.18, which\nmight be causing the problem. You can try upgrading to the latest version of\nSkyPilot to see if that solves the issue."}
{"title": "What is the VLLM discord?", "description": "", "answer": "The VLLM discord is a platform for communication and collaboration among VLLM\ncommunity members. It is used to discuss VLLM-related topics, share resources,\nand seek help from other members. It is likely that the weird error you are\nexperiencing can be resolved by reaching out to @Woosuk Kwon on the VLLM\ndiscord."}
{"title": "Controller error after ~21 hours: 'Controller's latest status is INIT; jobs will not be shown until it becomes UP'. What could be the issue?", "description": "I'm running into this error after having the controller be up for ~21 hours. I\ncan ssh into the machine and I also see all sorts of errors in the logs from\ngcp, though nothing obvious. Does anyone know what this could be? Also, is\nthere any way to restart the controller when it's in a bad state?", "answer": "To get the spot controller out of abnormal state, you can try `sky start -f\nsky-spot-controller-<hash>`."}
{"title": "AWS deployment timeout issue", "description": "", "answer": "The AWS deployment timeout issue could be caused by an incorrect SSH key\nconfiguration. You can try the SSH command mentioned in the provision.log to\nsee if it hangs. If it does, you may need to check the steps suggested in this\nthread: [AWS Deployment SSH Key Configuration](https://skypilot-\norg.slack.com/archives/C03J2KQQZSS/p1695345105283099?thread_ts=1695276478.957079&cid=C03J2KQQZSS)"}
{"title": "What's the best way to store AWS ECR credentials?", "description": "", "answer": "One way to store AWS ECR credentials is by specifying an executable command to\nthe environment variable, so that the password will be queried for each\nrecovery. This can be done by setting the `SKYPILOT_DOCKER_PASSWORD`\nenvironment variable to `$(aws ecr get-login-password --region us-west-2)`.\nAnother option is to pass the AWS secret key using the `--env` flag, but it\nmay not work when using a Docker image in the `resources` section."}
{"title": "Is there a Sky Server example for Mistral 7b?", "description": "", "answer": "Yes, you can find a YAML for deploying Mistral with Sky Serve in this gist:\n[link](https://gist.github.com/infwinston/f1b797b8fc200c0c165d2e9e3bdf057d).\nAdditionally, you can refer to the guide\n[here](https://docs.google.com/document/d/1vVmzLF-\nEkG3Moj-q47DQBGvFipK4PNfkz0V6LyaPstE/edit) for more information. The skyserve\nbranch is going to be merged soon to master."}
{"title": "When is it safe to close my computer after launching regular or managed spot jobs?", "description": "Does it matter if I am launching on an existing cluster or a new one? Do I\nneed to wait to see logs from my setup command, or is there an earlier point\nin time where a cloud instance has my yaml and I can log off?", "answer": "Assuming no `--detach-run` or `--detach-setup` is used, it is safe to ctrl-c\nafter the job’s submitted. One indicator of this is `INFO: Tip: use Ctrl-C to\nexit log streaming (task will not be killed).` If `--detach-setup` or\n`--detach-run` flags are used, you can safely ctrl-c to detach from logging\nwithout interrupting the setup process. To see the logs again after detaching,\nuse `sky logs`. To cancel setup, cancel the job via `sky cancel`. Useful for\nlong-running setup commands."}
{"title": "Running multiple containers using skypilot with docker-compose on a single GPU node", "description": "", "answer": "Yes, you can run multiple containers using skypilot with docker-compose on a\nsingle GPU node. In the `run` section of your skypilot YAML file, you can have\nmultiple `docker run` commands or use `docker compose up` if you have a\ndocker-compose file. Here's an example:\n\n    \n    \n      docker run ... &amp; \n      docker run ...```\n    \n    Alternatively, you can directly use `docker compose up` in the `run` section. See [example here](https://github.com/skypilot-org/skypilot/pull/2745/files)."}
{"title": "Skypilot stuck in infinite loop when reusing cluster on AWS", "description": "", "answer": "The issue you're experiencing with Skypilot getting stuck in an infinite loop\nwhen reusing a cluster on AWS is likely due to a lack of permission for the\n`ec2:DescribeInstances` action. This error occurs when the ray cluster on the\nremote VM encounters an unauthorized operation. To fix this, you need to\nensure that your AWS account has the necessary permissions. You can check your\nIAM user's permissions and make sure it has the `ec2:DescribeInstances`\npermission. Additionally, you can try using the latest `skypilot-nightly`\nversion, which includes a new provisioner that may resolve the issue. If\nyou're using a different IAM user or profile on your client, you'll need to\nensure that the VM has access to the correct AWS credentials. You can upload\nyour local static AWS credentials to the VM and let the service on the VM use\nthose credentials. If you're using an AWS profile, you may need to set the\n`AWS_PROFILE` environment variable in the VM through Skypilot configuration.\nPlease refer to the Skypilot documentation for more details."}
{"title": "Using Docker support with Google's Cloud Repository", "description": "", "answer": "The Docker support with Google's Cloud Repository is not officially supported\nyet. However, you can use a workaround by using `docker pull` and `docker run`\nin the `setup` and `run` sections of your task YAML. Here are the steps:\n\n  1. Run `gcloud auth configure-docker <your-location>.pkg.dev` locally.\n  2. Add the following section in your task YAML:\n\n    \n    \n      ~/.docker/config.json: ~/.docker/config.json```\n    \n    3. Launch your task using `sky launch`."}
{"title": "Availability of (spot) GPU quota across different clouds", "description": "Does anyone have experience they could share about the availability of (spot)\nGPU quota across different clouds?", "answer": "We did observe that the availability for spot on GCP is much better than on-\ndemand ones, especially for high-end GPUs, e.g., A100, A100-80GB. For on-\ndemand GPUs, it seems the availability is quite volatile, and it seems GCP has\nthe best availability among the big clouds, while some other clouds, such as\nOracle or IBM cloud can also have some GPUs available. We are also adding\nsupport for coreweave, fluidstack and runpod, so more GPUs to come."}
{"title": "What are the possible options or optimizations for reducing IO cost when using Trainings with pytorch and skypilot?", "description": "", "answer": "There are several options and optimizations you can try to reduce IO cost when\nusing Trainings with pytorch and skypilot:\n\n  1. Consider using S3 for storage, which is generally cheaper than EFS. The exact cost depends on your data size and workload's access patterns.\n  2. Use `COPY` mode in Sky Storage when using S3 with skypilot. This will maximize performance and reduce cost by pre-fetching the files to the VM's local disk.\n  3. To avoid egress costs when moving data across regions, place your data in the region where you typically launch your jobs. Alternatively, you can use Cloudflare R2, which does not charge any egress fee and is cheaper than S3.\n  4. EBS can also be considered as an option, especially if you are sticking to one region and zone.\n  5. Unfortunately, I'm not familiar with Ray Dataset, so I cannot comment on its cost.\n\nOverall, S3 is the most well-tested and recommended option for reducing IO\ncost."}
{"title": "What could be causing the SSH error and termination of the VM in skypilot integration with zenml?", "description": "", "answer": "The SSH error could be the cause of the termination of the VM. In some\nexperimental settings, a similar error was observed. It is recommended to\ncheck the SSH keys, manually run variants of the SSH command, and verify the\ninstance's SSH key in the AWS console."}
{"title": "How can I reuse an existing EC2 instance for the head instance in managed spot runs?", "description": "Is there a way to reuse one of the EC2 instances already running or specify\nresources for the head instance?", "answer": "For managed spot runs, you can specify the resources requirement of the\ncontroller in `~/.sky/config.yaml` file. You can set the number of CPUs\nrequired for the controller by adding the following configuration:\n\n    \n    \n      controller:\n        resources:\n          cpus: 4```\n    \n    To make the configuration take effect, you need to stop the current controller using `sky down sky-spot-controller-<hash>` and then launch it again using `sky spot launch` command. The controller name can be found in the output of `sky status` command.\n    \n    Additionally, for multiple managed spot jobs, they will reuse the same controller node, so the cost for it is amortized."}
{"title": "Is there a plan for supporting Databricks?", "description": "", "answer": "Yes, there is potential for supporting Databricks. Databricks is good at\nrunning data-intensive jobs, but there is a need for running compute-intensive\ntasks on small/medium-sized datasets. Integrating SkyPilot with Databricks can\nprovide the solution for this. It is already possible to run Ray on\nDatabricks, which opens up the possibility for further integration."}
{"title": "Does Skypilot currently only support the following regions? Can Skypilot support more regions?", "description": "", "answer": "Yes, Skypilot currently supports the following regions: 'centralus, eastus,\neastus2, northcentralus, southcentralus, westcentralus, westus, westus2,\nwestus3'. However, it can support more regions by adding them to the list of\nsupported regions in the code."}
{"title": "How can I specify the Ray/Python version that SkyPilot installs?", "description": "I'm launching my training jobs from a docker container using Skypilot, but it\nseems that the Ray cluster needs to be running the exact same Python/Ray\nversions. How can I specify the versions that Skypilot installs?", "answer": "You can create a new conda environment and start the Ray cluster head using\nthe new conda environment. Then, all the workers can connect to the cluster\nusing the new environment as well. Here's an example snippet:\n\n    \n    \n    setup: |\n      conda activate myray\n      if [ $? != \"0\" ]; then\n        conda create -n myray -y python=${PYTHON_VERSION}\n        conda activate myray\n      fi\n      pip install <https://s3-us-west-2.amazonaws.com/ray-wheels/master/${RAY_COMMIT}/ray-3.0.0.dev0-cp38-cp38-manylinux2014_x86_64.whl>\n    \n      # works for single node (if you need to do multiple nodes, we can talk more about it)\n      ray start --head\n    \n    run: |\n      # you should be able to connect to your own ray cluster in your program\n      conda activate myray\n      python myprogram.py"}
{"title": "Why is the 'ports' field unsupported in my YAML file when creating a cluster using SkyPilot?", "description": "", "answer": "The 'ports' field is currently unsupported in the version you are using. To\nget this feature, you can install SkyPilot from source or use the nightly\nbuild. After launching the cluster on AWS, the specified ports will be\naccessible outside the public cloud, and inbound rules will be automatically\nadded to the security group used by the instance."}
{"title": "Is storage required for using SkyServe with Azure?", "description": "", "answer": "Yes, currently SkyServe requires a storage cloud to be enabled. However, Azure\nBlob Storage is not supported yet. You can try using an AWS or GCP trial\naccount to enable S3 or GCS."}
{"title": "How to install Sky Serve in Skypilot?", "description": "", "answer": "To install Sky Serve in Skypilot, you can either install the latest nightly\nrelease using the command `pip install -U \"skypilot-nightly[all]\"`, or install\nfrom source by cloning the Skypilot repository and running `pip install\n\".[all]\"` in the cloned directory."}
{"title": "How does the optimizer work to get the lowest VM price across clouds?", "description": "", "answer": "The optimizer queries the service catalog, which contains instance types,\ntheir resources, and prices across clouds. It then runs an ILP (Integer Linear\nProgramming) to minimize cost. If you want more details, you can refer to the\npaper mentioned in the discussion thread: [Optimizer\nPaper](https://www.usenix.org/system/files/nsdi23-yang-zongheng.pdf).\nAdditionally, if you have specific questions about the code, you can ask for\nspecific places to refer to."}
{"title": "What is the difference between having `use_spot: True` in the yaml and running `sky spot launch` ?", "description": "", "answer": "The difference is that having `use_spot: True` in the yaml file launches an\nunmanaged spot task, which does not automatically recover from preemptions. On\nthe other hand, running `sky spot launch` or `sky.spot_launch()` launches a\nmanaged spot task, which enables auto-recovery."}
{"title": "How can I access the disk_size field from node_provider.create_node() in skypilot?", "description": "I am writing a node provider and I need to access the disk_size field from\nnode_provider.create_node(). However, the node_config Dict only has the\ninstance type in it. How can I get skypilot to pass in more info from the\nresources?", "answer": "To access the disk_size field from node_provider.create_node() in skypilot,\nyou can refer to this PR: [link](https://github.com/skypilot-\norg/skypilot/commit/a1b0bd309dc5d047f24dca4c94659d8b716f918b#diff-73daa838267cbf9c1f6491ea3898538cf07a4f9c3993103d6f7ae6d3fc4dc721R49).\nIt shows an example of how to pass additional information from the resources\nusing the provisioner API. Additionally, you may find this PR helpful for\nadding a new cloud using the new provisioner API:\n[link](https://github.com/skypilot-org/skypilot/pull/2880)."}
{"title": "What might be causing the 'RuntimeError: dictionary changed size during iteration' error?", "description": "", "answer": "The 'RuntimeError: dictionary changed size during iteration' error is caused\nby a bug that was introduced recently. A hot fix has been sent to address this\nissue. You can find the fix in this pull request:\n[link](https://github.com/skypilot-org/skypilot/pull/2876)."}
{"title": "Why did my spot job launch in AWS instead of GCP?", "description": "", "answer": "The spot controller is launched in AWS by default, and the spot job itself\nshould adhere to the optimizer table. The optimizer determined that AWS\ninstance/location is cheaper than any other choice in your search space. If\nyou want to change the spot controller location, you can configure it in the\nglobal config file. The intention of this design is that the spot controller\nis a global, long-lived VM shared among all spot jobs."}
{"title": "Why the need for continuous sync rather than let the next sky launch/exec handle it?", "description": "Understanding the purpose of continuous sync in skypilot", "answer": "The need for continuous sync arises when there is a requirement to sync\nproject code and run commands remotely with ssh. Continuous sync allows for\nreal-time updates and propagation of changes to the code on the cluster. It\ncan be useful in workflows like a jupyter + nbdev setup, where changes need to\nbe propagated to code in other notebooks/files on the cluster. The use of `sky\nexec` instead of manually running commands with ssh can also be considered as\nan alternative workflow."}
{"title": "How can I define custom VPC and images in Skypilot to suggest cost-effective cloud options?", "description": "", "answer": "To define custom VPC and images in Skypilot, you can use the `resources`\nsection in the task.yaml file. You can specify multiple resources using the\n`any_of` keyword and define the cloud provider and image ID for each resource.\nHere's an example:\n\n    \n    \n    resources:\n      any_of:\n        - cloud: aws\n          image_id:\n            us-east-1: ami-xxxx\n            us-east-2: ami-yyyy\n        - cloud: gcp\n          image_id: projects/xxx\n\nThis allows Skypilot to suggest cost-effective cloud options and pick the\ncustom image and VPC based on the specified criteria."}
{"title": "How can I specify specific resources for my job to run in any US region?", "description": "", "answer": "You can specify the specific resources for your job to run in any US region by\nusing the `any_of` syntax. Here is an example:\n\n    \n    \n      accelerators: T4:2\n      cloud: gcp\n      any_of:\n        - region: us-east5\n        - region: us-west1```"}
{"title": "How can you access Ray dashboard for a specific SkyServe head node if service replicas = 2?", "description": "", "answer": "Currently, accessing the Ray dashboard for a specific SkyServe head node when\nservice replicas are set to 2 is not supported. However, there is an open\nissue on GitHub to track this feature request:\n[link](https://github.com/skypilot-org/skypilot/issues/2859)"}
{"title": "Does sky pilot serve support Ray serve composite apps where >1 serve deployment are assembled in a pipeline?", "description": "How to work around a pipeline that may require Ray serve > the default Ray 2.4\nthat sky pilot provisions on a service node", "answer": "Yes, the dependency conflict can be fixed by using the PR mentioned in the\ndiscussion thread. You can find more details in the provided link."}
{"title": "How can I move a stopped spot instance to a new zone in GCP?", "description": "Is it possible to move a stopped spot instance to a new zone in GCP instead of\nrecreating it?", "answer": "Moving a stopped spot instance to a new zone in GCP is not officially\nsupported in skypilot. However, you can use the `gcloud` command-line tool to\nmanually move the instance to a new zone. Here is an example command:\n\n    \n    \n    gcloud compute instances move INSTANCE_NAME --zone=NEW_ZONE"}
{"title": "How to debug a skypilot job stuck in 'pending' state?", "description": "What steps can be taken to debug a skypilot job that is stuck in the 'pending'\nstate?", "answer": "To debug a skypilot job that is stuck in the 'pending' state, you can follow\nthese steps:\n\n  1. Check the job queue using the command `sky queue jupyter` to see the status of the job.\n  2. Check the logs of the job using the command `tail -f ~/sky_logs/sky-2023-12-08-15-56-32-623003/*` to look for any error messages or issues.\n  3. Run `ray job list` on the remote cluster to see the current status of the job.\n  4. Share the task yaml file with the skypilot team to help them reproduce the issue.\n  5. If the job was cloned from an existing cluster, it could be caused by stalled metadata on the remote cluster. In this case, the skypilot team can investigate and resolve the issue.\n\nIt is also recommended to provide any relevant information or error messages\nwhen seeking help with debugging a skypilot job."}
{"title": "How can I declaratively specify port ranges on the 'serve controller' in SkyServe?", "description": "I noticed that the examples do not specify port ranges and create a default\nrange. Can I customize the port ranges?", "answer": "Currently, it doesn't support custom port ranges for the controller. The load\nbalancing is done using a FastAPI running on your controller VM, which uses\nHTTP redirect to distribute traffic to replicas. However, there is an open\nissue <https://github.com/skypilot-org/skypilot/issues/2853> to keep track of\nthis requirement."}
{"title": "How to fix a connection refused error when running Ray?", "description": "", "answer": "To fix a connection refused error when running Ray, you can try the following\nsteps:\n\n  1. Relaunch the cluster using `sky launch -c cluster-name` to restore any defunct runtime.\n  2. SSH into the cluster and check if the Ray dashboard is up using `ray status` or `ps aux | grep -i ray`.\n  3. If the above steps don't work, try bringing up a new cluster and see if the error persists."}
{"title": "How can I overcome the permission issue when mounting an R2 to a mount point using sudo?", "description": "I am getting a permission error when trying to mount an R2 to a mount point\n/tools using sudo. How can I resolve this?", "answer": "To overcome the permission issue when mounting an R2 to a mount point using\nsudo, you can try adding `sudo` to the source code where the mounting command\nis constructed. Specifically, you can modify the\n`sky.data.storage.R2Store.mount_command()` function. However, please note that\nthis may change the permission requirements for the `/tools` directory, as it\nwill be mounted with sudo. Additionally, it is worth considering whether there\nis a specific need to set the directory at `/tools` instead of\n`/home/ubuntu/tools`."}
{"title": "Why does `sky stop` not work on GCP spot instances?", "description": "Is there a reason why `sky stop` is not allowed on GCP spot instances?", "answer": "The decision to not allow `sky stop` on GCP spot instances was made based on\nthe behavior of AWS. AWS throws an error when trying to stop a spot instance\nassociated with a one-time spot instance request. However, on GCP, the disks\nattached to spot instances are preserved and can be accessed and recovered.\nThis behavior is different from AWS. The issue has been filed to track this\nimprovement."}
{"title": "Can I choose a specific OS as a resource in SkyPilot?", "description": "Is it possible to choose CentOS 7 as the OS instead of the default Ubuntu?", "answer": "Yes, it is possible to choose a specific OS as a resource in SkyPilot. By\ndefault, SkyPilot uses Ubuntu as the OS. However, if you want to use CentOS 7,\nyou can choose a custom CentOS 7 AMI. When launching your job, make sure to\nhave the necessary tools installed in the CentOS AMI (e.g., wget, conda, pip3,\nsudo permission). You don't need to create the user 'ubuntu' in your CentOS\nAMI."}
{"title": "Is it possible to use francecentral Azure region in Sky pilot?", "description": "Is francecentral Azure region supported in Sky pilot?", "answer": "Yes, francecentral Azure region is supported in Sky pilot. However, the user\nwas experiencing an error because they were running the python command in the\nwrong folder. The command should be run inside the skypilot installation\nfolder."}
{"title": "What is the recommended way to run SkyPilot on Windows?", "description": "", "answer": "The recommended way to run SkyPilot on Windows is through WSL (Windows\nSubsystem for Linux). Windows support is tracked in this GitHub issue:\n[skypilot-org/skypilot/issues/1981](https://github.com/skypilot-\norg/skypilot/issues/1981)"}
{"title": "Is there any optimizer other than minimizing cost, such as latency and performance? If so, where can I choose a different optimizer?", "description": "", "answer": "Currently, only cost optimization is supported. However, there are plans for a\ncarbon-aware optimizer that aims to pick the 'greenest' VM for a specified\njob. To optimize for latency, you can manually specify regions in the job YAML\nfile. For performance, VMs with the same resource specification are expected\nto have similar raw performance. Network latency should not impact simulation\nperformance if the simulation does not need to communicate with your laptop.\nThe size of the design spec file and the simulation duration will determine\nthe impact of network latency during the upload."}
{"title": "Is the ability to run a DAG now supported by skypilot?", "description": "", "answer": "Yes, skypilot now supports running DAGs using spot pipelines. You can use the\n`spot launch` command on a sequential chain of tasks. The topography of the\ntasks can be controlled by writing them in the desired order in the YAML file.\nAdditionally, you can write logic to launch tasks on the cluster and control\nthe topography by waiting for task completion or creating an orchestrator task\nthat launches other tasks within the cluster."}
{"title": "Is there a way to retain the logs after the job is completed?", "description": "", "answer": "Yes, you can copy over the logs to a mounted object store or use `aws s3 sync`\nto sync the logs to an S3 bucket. Here's an example:\n\n    \n    \n    file_mounts:\n      /mylogs:\n        source: '<s3://romil-logs>'\n        mode: MOUNT\n    \n    setup: |\n      echo Setup\n    \n    run: |\n      echo MyRun\n      rsync -avz ~/sky_logs /mylogs\n    \n\nWith this, the logs will be stored in the mounted object store or the\nspecified S3 bucket."}
{"title": "Why is sky status not showing clusters and jobs running after installing sky cli on a new machine?", "description": "Do I need to do something else to make this work?", "answer": "There could be several reasons why sky status is not showing clusters and jobs\nrunning after installing sky cli on a new machine. One possibility is that the\nAWS account configuration is not properly set up on the new machine. Another\npossibility is that there may be an issue with the installation or\nconfiguration of the sky cli itself. It's also worth checking if there are any\nnetwork connectivity issues between the new machine and the target machine\nwhere the jobs are supposed to run. To troubleshoot the issue, you can try the\nfollowing steps:\n\n  1. Double-check the AWS account configuration on the new machine and make sure it matches the one used on the previous machine.\n  2. Verify that the sky cli is correctly installed and configured on the new machine.\n  3. Check for any error messages or logs that could indicate the cause of the issue.\n  4. Test the network connectivity between the new machine and the target machine to ensure there are no connectivity issues.\n\nIf the issue persists, it may be helpful to reach out to the skypilot\ncommunity or support for further assistance."}
{"title": "Is there a way to specify multiple regions?", "description": "Like if I want to run spot instance only in US regions?", "answer": "Yes, you can specify multiple regions when running spot instances. You can use\nthe `--region` flag in the AWS CLI or specify the region in your code when\nusing the AWS SDK. Here's an example using the AWS CLI:\n\n    \n    \n    aws ec2 run-instances --region us-west-1,us-west-2 --instance-type t2.micro --image-id ami-1234567890abcdef0"}
{"title": "How to set up a multi-node on k8s setup using SkyPilot?", "description": "", "answer": "To set up a multi-node on k8s setup using SkyPilot, you can use the\n`num_nodes` parameter in the `sky.Task` object. Make sure you are using\nSkyPilot version 0.4.1 or above. Here is an example script:\n\n    \n    \n    import sky\n    \n    task = sky.Task(\n        run='echo $SKYPILOT_NODE_IPS',\n        num_nodes=2\n    )\n    sky.launch(\n        task,\n        cluster_name='mycluster',\n    )\n\nThis script will spin up a cluster with 2 nodes and run the command `echo\n$SKYPILOT_NODE_IPS` on each node."}
{"title": "How can I resolve the issue with skypilot installing awscli and clobbering the aws command in a virtualenv?", "description": "", "answer": "One workaround for this issue is to create an alias for the aws command, such\nas `aws2`, and use that for anything SSO related. This can be done by running\nthe command `alias aws2=/usr/local/bin/aws`. However, this workaround may not\nbe ideal for everyone as it requires setting up the alias for each user.\nAnother option is to manually remove the [aws] extras from the skypilot\ninstallation, but this may not remove the awscli dependency. It's important to\nnote that this issue occurs when skypilot is installed and overrides the\nexisting aws command with aws cli v1, even if aws cli v2 was already\ninstalled."}
{"title": "How can I upload files from a local directory to an existing S3 bucket using the Python API?", "description": "", "answer": "To upload files from a local directory to an existing S3 bucket using the\nPython API, you can use the `sky` library. However, note that the library\ncurrently does not support mounting a specific sub-path on a bucket. The root\nof the bucket is mounted at the specified mount point. To achieve your goal,\nyou can change the path where you write your checkpoints such that it is\nrelative to the root of the mount point. Here's an example snippet:\n\n    \n    \n    import contextlib\n    import sky\n    \n    cluster = \"my-cluster\"\n    \n    with contextlib.suppress(ValueError):\n        sky.down(cluster)\n    \n    # Create a task\n    \n    # Set resources\n    \n    # Set storage mounts\n    \n    # Launch the task"}
{"title": "Is the `workdir` in the yaml file relative to the user's current working directory or relative to the yaml file itself?", "description": "", "answer": "Yes, the `workdir` specified in the yaml file is relative to the user's\ncurrent working directory. This behavior is similar to python or bash scripts,\nwhere the relative path in the file is relative to the place where the command\nis run, rather than relative to the script/yaml file itself."}
{"title": "Is there a way to configure managed spot instances to automatically shut down upon launch?", "description": "", "answer": "Yes, for managed spot instances, they are automatically shut down upon job\nexit. To launch a managed spot instance and have it shut down after executing\na command, you can use the `sky spot launch` command followed by the command\nyou want to execute. For example:\n\n    \n    \n    sky spot launch -- echo hi\n\nThis will launch the spot instance, execute the command `echo hi`, and then\nshut down the instance."}
{"title": "Can the Ray dashboard be enabled in SkyPilot for observability purposes?", "description": "", "answer": "Yes, the Ray dashboard can be enabled in SkyPilot for observability purposes.\nBy default, it should be running on the head node. To enable it, you can use\nthe following code snippet:\n\n    \n    \n    remote_host=$(sky status | awk 'NR==3{print $1}')\n    ssh -fNT -L \"${RAY_DASHBOARD_PORT}:localhost:${RAY_DASHBOARD_PORT}\" \"$remote_host\" #&gt;/dev/null 2&gt;&amp;1```"}
{"title": "How to debug a skypilot job stuck in 'pending' state?", "description": "What steps can be taken to debug a skypilot job that is stuck in the 'pending'\nstate?", "answer": "To debug a skypilot job that is stuck in the 'pending' state, you can follow\nthese steps:\n\n  1. Check the job queue using the command `sky queue jupyter` to see the status of the job.\n  2. Check the logs of the job using the command `tail -f ~/sky_logs/sky-2023-12-08-15-56-32-623003/*` to look for any error messages or issues.\n  3. Run `ray job list` on the remote cluster to see the current status of the job.\n  4. Share the task yaml file with the skypilot team to help them reproduce the issue.\n  5. If the job was cloned from an existing cluster, it could be caused by stalled metadata on the remote cluster. In this case, the skypilot team can investigate and resolve the issue.\n\nIt is also recommended to provide any relevant information or error messages\nwhen seeking help with debugging a skypilot job."}
{"title": "How can I move a stopped spot instance to a new zone in GCP?", "description": "Is it possible to move a stopped spot instance to a new zone in GCP instead of\nrecreating it?", "answer": "Moving a stopped spot instance to a new zone in GCP is not officially\nsupported in skypilot. However, you can use the `gcloud` command-line tool to\nmanually move the instance to a new zone. Here is an example command:\n\n    \n    \n    gcloud compute instances move INSTANCE_NAME --zone=NEW_ZONE"}
{"title": "Does sky pilot serve support Ray serve composite apps where >1 serve deployment are assembled in a pipeline?", "description": "How to work around a pipeline that may require Ray serve > the default Ray 2.4\nthat sky pilot provisions on a service node", "answer": "Yes, the dependency conflict can be fixed by using the PR mentioned in the\ndiscussion thread. You can find more details in the provided link."}
{"title": "How can you access Ray dashboard for a specific SkyServe head node if service replicas = 2?", "description": "", "answer": "Currently, accessing the Ray dashboard for a specific SkyServe head node when\nservice replicas are set to 2 is not supported. However, there is an open\nissue on GitHub to track this feature request:\n[link](https://github.com/skypilot-org/skypilot/issues/2859)"}
{"title": "How can I specify specific resources for my job to run in any US region?", "description": "", "answer": "You can specify the specific resources for your job to run in any US region by\nusing the `any_of` syntax. Here is an example:\n\n    \n    \n      accelerators: T4:2\n      cloud: gcp\n      any_of:\n        - region: us-east5\n        - region: us-west1```"}
{"title": "How can I define custom VPC and images in Skypilot to suggest cost-effective cloud options?", "description": "", "answer": "To define custom VPC and images in Skypilot, you can use the `resources`\nsection in the task.yaml file. You can specify multiple resources using the\n`any_of` keyword and define the cloud provider and image ID for each resource.\nHere's an example:\n\n    \n    \n    resources:\n      any_of:\n        - cloud: aws\n          image_id:\n            us-east-1: ami-xxxx\n            us-east-2: ami-yyyy\n        - cloud: gcp\n          image_id: projects/xxx\n\nThis allows Skypilot to suggest cost-effective cloud options and pick the\ncustom image and VPC based on the specified criteria."}
{"title": "Why the need for continuous sync rather than let the next sky launch/exec handle it?", "description": "Understanding the purpose of continuous sync in skypilot", "answer": "The need for continuous sync arises when there is a requirement to sync\nproject code and run commands remotely with ssh. Continuous sync allows for\nreal-time updates and propagation of changes to the code on the cluster. It\ncan be useful in workflows like a jupyter + nbdev setup, where changes need to\nbe propagated to code in other notebooks/files on the cluster. The use of `sky\nexec` instead of manually running commands with ssh can also be considered as\nan alternative workflow."}
{"title": "How does the optimizer work to get the lowest VM price across clouds?", "description": "", "answer": "The optimizer queries the service catalog, which contains instance types,\ntheir resources, and prices across clouds. It then runs an ILP (Integer Linear\nProgramming) to minimize cost. If you want more details, you can refer to the\npaper mentioned in the discussion thread: [Optimizer\nPaper](https://www.usenix.org/system/files/nsdi23-yang-zongheng.pdf).\nAdditionally, if you have specific questions about the code, you can ask for\nspecific places to refer to."}
{"title": "Why did my spot job launch in AWS instead of GCP?", "description": "", "answer": "The spot controller is launched in AWS by default, and the spot job itself\nshould adhere to the optimizer table. The optimizer determined that AWS\ninstance/location is cheaper than any other choice in your search space. If\nyou want to change the spot controller location, you can configure it in the\nglobal config file. The intention of this design is that the spot controller\nis a global, long-lived VM shared among all spot jobs."}
{"title": "What might be causing the 'RuntimeError: dictionary changed size during iteration' error?", "description": "", "answer": "The 'RuntimeError: dictionary changed size during iteration' error is caused\nby a bug that was introduced recently. A hot fix has been sent to address this\nissue. You can find the fix in this pull request:\n[link](https://github.com/skypilot-org/skypilot/pull/2876)."}
{"title": "How to Resolve SkyPilot's INIT Status and FetchIPError After Internet Disconnection?", "description": "I was relaunching a stopped cluster using SkyPilot when my internet connection\nwas interrupted. Now, SkyPilot seems to be stuck in limbo, and the cluster\nstatus remains 'INIT' even after 20 minutes. I've tried to `sky launch`\nmultiple times, but I keep encountering a `sky.exceptions.FetchIPError`.\nHere's the command I used and the error output:\n\n    \n    \n    $ sky launch -c spot-jupyter --use-spot jupyter.yaml\n    # ... error output ...\n    subprocess.CalledProcessError: Command 'ray get-head-ip '/var/folders/z1/yww43t6d0vl6qp45379h2n540000gn/T/tmpdu3ug4w9'' returned non-zero exit status 1.\n    # ... more error output ...\n    sky.exceptions.FetchIPError\n\nHow can I resolve this issue?", "answer": "Zhanghao Wu from SkyPilot suggested that I try `sky launch` again on the\ncluster, as it often fixes the `INIT` status. However, when I attempted this,\nit resulted in the same IPError each time. Zhanghao then inquired if I had\nchecked the GCP console for preemption issues, which I had not. He also\nmentioned that a more robust GCP provisioner is being developed for SkyPilot,\nwhich should address many current issues, including the one I'm facing.\nAdditionally, Zongheng Yang noted that stopping GCP spot instances has been\nadded in a recent pull request under review."}
{"title": "How can I access the disk_size field from node_provider.create_node() in skypilot?", "description": "I am writing a node provider and I need to access the disk_size field from\nnode_provider.create_node(). However, the node_config Dict only has the\ninstance type in it. How can I get skypilot to pass in more info from the\nresources?", "answer": "To access the disk_size field from node_provider.create_node() in skypilot,\nyou can refer to this PR: [link](https://github.com/skypilot-\norg/skypilot/commit/a1b0bd309dc5d047f24dca4c94659d8b716f918b#diff-73daa838267cbf9c1f6491ea3898538cf07a4f9c3993103d6f7ae6d3fc4dc721R49).\nIt shows an example of how to pass additional information from the resources\nusing the provisioner API. Additionally, you may find this PR helpful for\nadding a new cloud using the new provisioner API:\n[link](https://github.com/skypilot-org/skypilot/pull/2880)."}
{"title": "Is storage required for using SkyServe with Azure?", "description": "", "answer": "Yes, currently SkyServe requires a storage cloud to be enabled. However, Azure\nBlob Storage is not supported yet. You can try using an AWS or GCP trial\naccount to enable S3 or GCS."}
{"title": "Are there plans to align Skypilot with the main Ray project?", "description": "Can Skypilot align with the main Ray project to make sky serve and Ray serve\nmore interoperable?", "answer": "Yes, there are plans to align Skypilot with the main Ray project. This will\nmake sky serve and Ray serve more interoperable. By aligning with the main Ray\nproject, Skypilot can leverage the latest features and improvements from Ray,\nensuring compatibility and enhancing interoperability between the two\nprojects. This alignment will benefit users who use both Skypilot and Ray\nserve, enabling them to seamlessly integrate and leverage the functionalities\nof both platforms."}
{"title": "Has Skypilot considered integrating with SLURM?", "description": "I noticed that the job scheduler in Skypilot is similar to the `srun` CLI used\nin SLURM. Has the Skypilot team considered integrating with SLURM?", "answer": "Yes, the Skypilot team has considered integrating with SLURM. However, they\nhave prioritized supporting Kubernetes for on-prem deployments due to its\npopularity and comprehensive feature set. They are open to contributions\nadding support for SLURM. SLURM is known for its ability to share finite\nresources in an on-prem HPC cluster, while Kubernetes is designed for\ndedicated usage of resources provisioned from an 'infinite' pool."}
{"title": "Are there any plans to implement the ability to choose Custom V-net and custom images in Azure?", "description": "The user is asking if there are any plans to implement the ability to choose\nCustom V-net and custom images in Azure.", "answer": "Yes, there are plans to implement the ability to choose Custom V-net and\ncustom images in Azure. However, it is currently not available. You can check\nfor updates on the Azure roadmap or reach out to the Azure support team for\nmore information."}
{"title": "How long does it typically take to provision the Vicuna model following the Sky serve tutorial?", "description": "I'm trying to provision the Vicuna model following the Sky serve tutorial, but\nit has been running for about an hour and a half. I have successfully run the\n'Hello, SkyServe!' example. Any idea how long it usually takes to provision\nthe model?", "answer": "The provisioning time for the Vicuna model can vary depending on various\nfactors such as the availability of resources and the chosen accelerator.\nHowever, it is not uncommon for the provisioning process to take more than an\nhour. If the provisioning process is taking longer than expected, it is\nrecommended to check the logs for any errors or issues. Additionally, it may\nbe worth considering using a different accelerator, such as V100 or spot A100,\nif the OnDemand A100 is scarce."}
